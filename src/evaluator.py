"""
Priori Judgment è¯„ä¼°å™¨ - ä¸¤é˜¶æ®µæ¨ç†
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Dict
from tqdm import tqdm
from dataclasses import dataclass

from .prompts import PromptTemplates
from .metrics import Metrics
from .data_loader import Sample


@dataclass
class EvalResult:
    """è¯„ä¼°ç»“æœ"""
    id: str
    question: str
    prediction: str
    gold_answers: List[str]
    mode: str  # "rag" or "closedbook"
    priori_output: str


class PrioriJudgmentEvaluator:
    """
    ä¸¤é˜¶æ®µæ¨ç†è¯„ä¼°å™¨
    
    å®ç°è®ºæ–‡çš„ Priori Judgment baselineï¼š
    Stage 1: Priori Judgment with Top-1 retrieval
    Stage 2: Fallback to Closed-book if "Unknown"
    """
    
    # ä»»åŠ¡ç±»å‹æ˜ å°„
    TASK_TYPES = {
        'nq': 'open_qa',
        'trivia': 'open_qa',
        'triviaqa': 'open_qa',
        'webqa': 'open_qa',
        'truthfulqa': 'long_form',
        'factkg': 'fact_checking'
    }
    
    def __init__(
        self,
        model_name: str = "NousResearch/Meta-Llama-3-8B-Instruct",
        device: str = "cuda",
        max_new_tokens: int = 30
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_name: HuggingFace æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è®¾å¤‡ (cuda/cpu)
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆè®ºæ–‡è®¾ç½®ä¸º 30ï¼‰
        """
        self.device = device if torch.cuda.is_available() else "cpu"
        self.max_new_tokens = max_new_tokens
        
        print(f"ğŸ”§ Loading model: {model_name}")
        print(f"   Device: {self.device}")
        
        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side="left"  # é‡è¦ï¼šå·¦ä¾§å¡«å……ç”¨äºç”Ÿæˆ
        )
        
        # è®¾ç½® pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,  # ä½¿ç”¨ FP16 èŠ‚çœæ˜¾å­˜
            device_map="auto",          # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
            trust_remote_code=True,
            low_cpu_mem_usage=True      # ä¼˜åŒ– CPU å†…å­˜ä½¿ç”¨
        )
        self.model.eval()
        
        print(f"âœ… Model loaded")
    
    @torch.no_grad()
    def generate(self, prompt: str) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        ä½¿ç”¨ Greedy Decodingï¼ˆtemperature=0ï¼Œæ— é‡‡æ ·ï¼‰
        ç¡®ä¿ç»“æœå¯å¤ç°
        
        Args:
            prompt: è¾“å…¥ prompt
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆä¸åŒ…å« promptï¼‰
        """
        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024
        ).to(self.device)
        
        # Generate
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=self.max_new_tokens,
            do_sample=False,  # Greedy decodingï¼ˆå…³é”®ï¼‰
            pad_token_id=self.tokenizer.eos_token_id,
            eos_token_id=self.tokenizer.eos_token_id
        )
        
        # Decodeï¼ˆåªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        input_length = inputs['input_ids'].shape[1]
        generated_ids = outputs[0][input_length:]
        generated_text = self.tokenizer.decode(
            generated_ids,
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def is_unknown(self, text: str) -> bool:
        """
        æ£€æµ‹æ˜¯å¦åŒ…å« "Unknown" æ‹’ç»ä¿¡å·
        
        é²æ£’å¤„ç†å¤šç§æ‹’ç»å›ç­”çš„æ¨¡å¼ï¼š
        - "Unknown"
        - "I don't know"
        - "I'm not sure"
        - "Cannot answer"
        - ç­‰ç­‰
        
        Args:
            text: æ¨¡å‹è¾“å‡ºæ–‡æœ¬
        
        Returns:
            True å¦‚æœæ£€æµ‹åˆ°æ‹’ç»ä¿¡å·
        """
        text_lower = text.lower().strip()
        
        # 1. ç²¾ç¡®åŒ¹é…
        if text_lower == "unknown":
            return True
        
        # 2. å¼€å¤´åŒ¹é…
        if text_lower.startswith("unknown"):
            return True
        
        # 3. å¸¸è§æ‹’ç»æ¨¡å¼
        unknown_patterns = [
            "unknown",
            "i don't know",
            "i do not know",
            "i'm not sure",
            "i am not sure",
            "cannot answer",
            "can't answer",
            "unable to answer",
            "no information",
            "not enough information"
        ]
        
        for pattern in unknown_patterns:
            if pattern in text_lower:
                return True
        
        return False
    
    def evaluate_sample(self, sample: Sample) -> EvalResult:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬ï¼ˆä¸¤é˜¶æ®µæ¨ç†ï¼‰
        
        Stage 1: Priori Judgment
        - ä½¿ç”¨ Top-1 æ£€ç´¢ä¸Šä¸‹æ–‡
        - åˆ¤æ–­æ˜¯å¦èƒ½å›ç­”
        
        Stage 2: Fallback (if needed)
        - å¦‚æœè¾“å‡º "Unknown"ï¼Œå›é€€åˆ° Closed-book
        
        Args:
            sample: è¯„ä¼°æ ·æœ¬
        
        Returns:
            è¯„ä¼°ç»“æœ
        """
        task_type = self.TASK_TYPES[sample.dataset]
        
        # === Stage 1: Priori Judgment ===
        if task_type == "fact_checking":
            priori_prompt = PromptTemplates.priori_judgment_fact(
                sample.question, sample.top1_context
            )
        else:
            priori_prompt = PromptTemplates.priori_judgment_qa(
                sample.question, sample.top1_context
            )
        
        priori_output = self.generate(priori_prompt)
        
        # === Stage 2: Check Unknown & Fallback ===
        if self.is_unknown(priori_output):
            # Fallback to closed-book
            if task_type == "fact_checking":
                cb_prompt = PromptTemplates.closedbook_fact(sample.question)
            elif task_type == "long_form":
                cb_prompt = PromptTemplates.closedbook_qa_long(sample.question)
            else:
                cb_prompt = PromptTemplates.closedbook_qa_short(sample.question)
            
            final_answer = self.generate(cb_prompt)
            mode = "closedbook"
        else:
            # ä½¿ç”¨ priori output ç›´æ¥ä½œä¸ºç­”æ¡ˆ
            final_answer = priori_output
            mode = "rag"
        
        return EvalResult(
            id=sample.id,
            question=sample.question,
            prediction=final_answer,
            gold_answers=sample.answers,
            mode=mode,
            priori_output=priori_output
        )
    
    def evaluate_dataset(self, samples: List[Sample]) -> Dict:
        """
        è¯„ä¼°æ•´ä¸ªæ•°æ®é›†
        
        Args:
            samples: æ ·æœ¬åˆ—è¡¨
        
        Returns:
            åŒ…å« metrics, results, mode_distribution çš„å­—å…¸
        """
        results = []
        
        print(f"ğŸ”„ Evaluating {len(samples)} samples...")
        for sample in tqdm(samples, desc="Processing"):
            result = self.evaluate_sample(sample)
            results.append(result)
        
        # è®¡ç®—æŒ‡æ ‡
        task_type = self.TASK_TYPES[samples[0].dataset]
        metrics = self._compute_metrics(results, task_type)
        
        # ç»Ÿè®¡æ¨¡å¼åˆ†å¸ƒ
        mode_counts = {'rag': 0, 'closedbook': 0}
        for r in results:
            mode_counts[r.mode] += 1
        
        return {
            'metrics': metrics,
            'results': results,
            'mode_distribution': mode_counts
        }
    
    def _compute_metrics(
        self,
        results: List[EvalResult],
        task_type: str
    ) -> Dict[str, float]:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©åˆé€‚çš„æŒ‡æ ‡ï¼š
        - fact_checking: Accuracy
        - long_form: F1 + ROUGE-L
        - open_qa: Span EM
        
        Args:
            results: è¯„ä¼°ç»“æœåˆ—è¡¨
            task_type: ä»»åŠ¡ç±»å‹
        
        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        if task_type == "fact_checking":
            # FactKG: Accuracy
            scores = [
                Metrics.compute_accuracy(r.prediction, r.gold_answers)
                for r in results
            ]
            return {'accuracy': sum(scores) / len(scores)}
        
        elif task_type == "long_form":
            # TruthfulQA: F1 + ROUGE-Lï¼ˆé‡è¦ï¼šè®ºæ–‡æŠ¥å‘Šäº†ä¸¤ä¸ªæŒ‡æ ‡ï¼‰
            f1_scores = [
                Metrics.compute_f1(r.prediction, r.gold_answers)
                for r in results
            ]
            rouge_scores = [
                Metrics.compute_rouge_l(r.prediction, r.gold_answers)
                for r in results
            ]
            return {
                'f1': sum(f1_scores) / len(f1_scores),
                'rouge_l': sum(rouge_scores) / len(rouge_scores)
            }
        
        else:
            # Open-domain QA: Span EM
            scores = [
                Metrics.compute_span_em(r.prediction, r.gold_answers)
                for r in results
            ]
            return {'span_em': sum(scores) / len(scores)}