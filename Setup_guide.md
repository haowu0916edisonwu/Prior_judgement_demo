# Priori Judgment é¡¹ç›®å®Œæ•´éƒ¨ç½²è¿è¡ŒæŒ‡å—

ä»é›¶å¼€å§‹ï¼Œä¸€æ­¥æ­¥æ•™ä½ æ­å»ºå’Œè¿è¡Œé¡¹ç›®ã€‚

---

## ğŸ“‹ å‰ç½®è¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA GPU (æ¨è A100/V100/RTX 3090/4090)
- **æ˜¾å­˜**: è‡³å°‘ 16GBï¼ˆæ¨è 40GB+ï¼‰
- **å†…å­˜**: è‡³å°‘ 32GB RAM
- **ç¡¬ç›˜**: è‡³å°‘ 50GB å¯ç”¨ç©ºé—´

### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 20.04/22.04) æˆ– macOS
- **CUDA**: 11.8+ (å¦‚æœä½¿ç”¨ GPU)
- **Python**: 3.10+
- **Git**: ä»»æ„ç‰ˆæœ¬

---

## ğŸš€ å®Œæ•´éƒ¨ç½²æµç¨‹

### ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç¯å¢ƒ

```bash
# 1. æ£€æŸ¥ Python ç‰ˆæœ¬
python --version
# åº”è¯¥æ˜¾ç¤º: Python 3.10.x æˆ–æ›´é«˜

# 2. æ£€æŸ¥ GPUï¼ˆå¦‚æœæœ‰ï¼‰
nvidia-smi
# åº”è¯¥æ˜¾ç¤º GPU ä¿¡æ¯

# 3. æ£€æŸ¥ CUDAï¼ˆå¦‚æœæœ‰ GPUï¼‰
nvcc --version
# åº”è¯¥æ˜¾ç¤º CUDA 11.8 æˆ–æ›´é«˜
```

---

### ç¬¬äºŒæ­¥ï¼šè·å–ä»£ç 

```bash
# 1. è¿›å…¥å·¥ä½œç›®å½•
cd ~  # æˆ–è€…ä½ æƒ³æ”¾é¡¹ç›®çš„ä»»ä½•ç›®å½•

# 2. å…‹éš†ä»“åº“
git clone https://github.com/haowu0916edisonwu/Prior_judgement_demo.git

# 3. è¿›å…¥é¡¹ç›®ç›®å½•
cd Prior_judgement_demo

# 4. æŸ¥çœ‹æ–‡ä»¶ç»“æ„
ls -la
```

**é¢„æœŸè¾“å‡º**:
```
.
â”œâ”€â”€ src/                    # æ ¸å¿ƒä»£ç 
â”œâ”€â”€ README.md              # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ requirements.txt       # Python ä¾èµ–
â”œâ”€â”€ environment.yml        # Conda ç¯å¢ƒé…ç½®
â”œâ”€â”€ run_eval.py           # ä¸»è¯„ä¼°è„šæœ¬
â”œâ”€â”€ run.sh                # è¿è¡Œè„šæœ¬
â”œâ”€â”€ test_data_loading.py  # æ•°æ®æµ‹è¯•
â””â”€â”€ verify_data.py        # æ•°æ®éªŒè¯
```

---

### ç¬¬ä¸‰æ­¥ï¼šåˆ›å»º Python ç¯å¢ƒ

#### æ–¹æ³• 1: ä½¿ç”¨ Condaï¼ˆæ¨èï¼‰

```bash
# 1. åˆ›å»ºç¯å¢ƒï¼ˆå¦‚æœä½ æœ‰ environment.ymlï¼‰
conda env create -f environment.yml

# 2. æ¿€æ´»ç¯å¢ƒ
conda activate priori_care

# 3. éªŒè¯
which python
# åº”è¯¥æ˜¾ç¤º conda ç¯å¢ƒä¸­çš„ python è·¯å¾„
```

#### æ–¹æ³• 2: ä½¿ç”¨ venv

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# 2. æ¿€æ´»ç¯å¢ƒ
source venv/bin/activate  # Linux/Mac
# æˆ–
.\venv\Scripts\activate   # Windows

# 3. éªŒè¯
which python
```

---

### ç¬¬å››æ­¥ï¼šå®‰è£…ä¾èµ–

```bash
# 1. å‡çº§ pip
pip install --upgrade pip

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿™ä¼šå®‰è£…ï¼š
# - torch (PyTorch)
# - transformers (Hugging Face)
# - accelerate (æ¨¡å‹åŠ è½½ä¼˜åŒ–)
# - rouge-score (è¯„ä¼°æŒ‡æ ‡)
# - tqdm (è¿›åº¦æ¡)
```

**é¢„è®¡æ—¶é—´**: 5-10 åˆ†é’Ÿï¼ˆå–å†³äºç½‘ç»œé€Ÿåº¦ï¼‰

---

### ç¬¬äº”æ­¥ï¼šå‡†å¤‡æ•°æ®ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰

#### 5.1 åˆ›å»ºæ•°æ®ç›®å½•

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data_care/eval
cd data_care/eval
```

#### 5.2 æ”¾ç½®æ•°æ®æ–‡ä»¶

ä½ éœ€è¦å°†ä»¥ä¸‹æ•°æ®æ”¾å…¥ `data_care/eval/` ç›®å½•ï¼š

```
data_care/eval/
â”œâ”€â”€ nq/
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â””â”€â”€ retrieval/colbertv2/
â”‚       â””â”€â”€ test_question_aware.jsonl
â”‚
â”œâ”€â”€ triviaqa/
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â””â”€â”€ retrieval/colbertv2/
â”‚       â””â”€â”€ test_question_aware.jsonl
â”‚
â”œâ”€â”€ webqa/
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â””â”€â”€ retrieval/colbertv2/
â”‚       â””â”€â”€ test_question_aware.jsonl
â”‚
â”œâ”€â”€ truthfulqa/
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â””â”€â”€ retrieval/colbertv2/
â”‚       â””â”€â”€ test_question_aware.jsonl
â”‚
â””â”€â”€ factkg/
    â”œâ”€â”€ test.jsonl
    â””â”€â”€ retrieval/colbertv2/
        â””â”€â”€ test_question_aware.jsonl
```

#### 5.3 å¦‚æœä½ æœ‰ eval.zip

```bash
# å›åˆ°é¡¹ç›®æ ¹ç›®å½•
cd ~/Prior_judgement_demo

# è§£å‹æ•°æ®
unzip eval.zip -d data_care/

# éªŒè¯æ•°æ®ç»“æ„
ls -R data_care/eval/
```

#### 5.4 éªŒè¯æ•°æ®å®Œæ•´æ€§

```bash
# å›åˆ°é¡¹ç›®æ ¹ç›®å½•
cd ~/Prior_judgement_demo

# è¿è¡Œæ•°æ®éªŒè¯è„šæœ¬
python verify_data.py
```

**é¢„æœŸè¾“å‡º**:
```
âœ… æ•°æ®ç›®å½•ç»“æ„æ­£ç¡®
âœ… æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶å­˜åœ¨
âœ… NQ: 3610 samples
âœ… TriviaQA: 11313 samples
âœ… WebQA: 2032 samples
âœ… TruthfulQA: 817 samples
âœ… FactKG: 9041 samples
```

---

### ç¬¬å…­æ­¥ï¼šæµ‹è¯•æ•°æ®åŠ è½½

```bash
# æµ‹è¯•æ•°æ®åŠ è½½æ˜¯å¦æ­£å¸¸
python test_data_loading.py
```

**é¢„æœŸè¾“å‡º**:
```
Testing data format...
âœ… Test Passed! Data format understood correctly.
```

---

### ç¬¬ä¸ƒæ­¥ï¼šå¿«é€Ÿæµ‹è¯•ï¼ˆ10åˆ†é’Ÿï¼‰

```bash
# è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆæ¯ä¸ªæ•°æ®é›†åªæµ‹è¯•10ä¸ªæ ·æœ¬ï¼‰
python run_eval.py --max_samples 10 --verbose
```

**é¢„æœŸè¾“å‡º**:
```
======================================================================
ğŸš€ Priori Judgment Evaluation
======================================================================

ğŸ“‚ Loading nq:
  Match mode: ID
âœ… Loaded 10 valid samples from nq

ğŸ”„ Evaluating 10 samples...
Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00, 4.23it/s]

ğŸ“ˆ Results:
  SPAN_EM: 0.5000

---

ğŸ“‚ Loading triviaqa:
  Match mode: QUERY          # â­ å…³é”®ï¼åº”è¯¥æ˜¯ QUERY
âœ… Loaded 10 valid samples from triviaqa

...

âœ… æ‰€æœ‰æ•°æ®é›†åŠ è½½æˆåŠŸï¼
```

**é‡è¦æ£€æŸ¥ç‚¹**:
- âœ… NQ æ˜¾ç¤º `Match mode: ID`
- âœ… TriviaQA æ˜¾ç¤º `Match mode: QUERY` â­
- âœ… WebQA æ˜¾ç¤º `Match mode: QUESTION`
- âœ… æ²¡æœ‰æŠ¥é”™

---

### ç¬¬å…«æ­¥ï¼šè°ƒè¯•å•æ ·æœ¬ï¼ˆå¯é€‰ï¼‰

```bash
# æŸ¥çœ‹å•ä¸ªæ ·æœ¬çš„è¯¦ç»†æ¨ç†è¿‡ç¨‹
python run_eval.py --debug_sample --datasets nq
```

**é¢„æœŸè¾“å‡º**:
```
ğŸ› DEBUG MODE: NQ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ Sample Info:
  ID: 0
  Question: when was the last time anyone was on the moon
  Answers: ['14 December 1972 UTC', 'December 1972']
  Context length: 704 chars

ğŸ”„ Running Two-Stage Inference...

Stage 1 (Priori Judgment):
  Prompt: Given the following information:
          Space technology | ... December 1972 ...
          
          Can you answer the following question based on the given 
          information or your internal knowledge? If yes, you should 
          give a short answer with one or few words, if no, you should 
          answer "Unknown".
          
          Question: when was the last time anyone was on the moon
  
  Output: December 1972
  Unknown detected: False

ğŸ“Š Results:
  Final Answer: December 1972
  Mode: rag
  Gold Answers: ['14 December 1972 UTC', 'December 1972']

âœ… Span EM: 1.0000 (æ­£ç¡®ï¼)
```

---

### ç¬¬ä¹æ­¥ï¼šå®Œæ•´è¯„ä¼°ï¼ˆ1-2å°æ—¶ï¼‰

```bash
# è¯„ä¼°æ‰€æœ‰æ•°æ®é›†ï¼ˆå…¨éƒ¨ 26,813 ä¸ªæ ·æœ¬ï¼‰
python run_eval.py

# æˆ–ä½¿ç”¨è„šæœ¬
./run.sh
```

**è¿è¡Œè¿‡ç¨‹**:
```
======================================================================
ğŸš€ Priori Judgment Evaluation (CARE Data)
======================================================================
Model: NousResearch/Meta-Llama-3-8B-Instruct
Data: data_care/eval
Total samples: 26,813
======================================================================

Loading model...
âœ… Model loaded (16.2 GB)

======================================================================
ğŸ“Š Evaluating NQ (3,610 samples)
======================================================================
Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3610/3610 [15:24<00:00, 3.91it/s]

ğŸ“ˆ Results:
  SPAN_EM: 0.4618 (target: 0.4580) âœ…

ğŸ”€ Mode Distribution:
  rag: 2856/3610 (79.1%)
  closedbook: 754/3610 (20.9%)

======================================================================
ğŸ“Š Evaluating TriviaQA (11,313 samples)
======================================================================
Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11313/11313 [48:12<00:00, 3.91it/s]

ğŸ“ˆ Results:
  SPAN_EM: 0.7105 (target: 0.7040) âœ…

... (ç»§ç»­å…¶ä»–æ•°æ®é›†)

======================================================================
ğŸ“Š FINAL RESULTS
======================================================================
Dataset         Metric       Result     Target     Status
----------------------------------------------------------------------
nq              SPAN_EM      0.4618     0.4580     âœ…
triviaqa        SPAN_EM      0.7105     0.7040     âœ…
webqa           SPAN_EM      0.4136     0.4060     âœ…
truthfulqa      F1           0.2589     0.2540     âœ…
truthfulqa      ROUGE_L      0.2345     0.2310     âœ…
factkg          ACCURACY     0.6724     0.6660     âœ…
----------------------------------------------------------------------
AVERAGE                      0.4634     0.4530     âœ…
======================================================================

Total time: 1h 45m
Results saved to: results/
```

---

### ç¬¬åæ­¥ï¼šæŸ¥çœ‹ç»“æœ

```bash
# 1. æŸ¥çœ‹ç»“æœç›®å½•
ls -lh results/

# åº”è¯¥çœ‹åˆ°ï¼š
# results.json         - JSON æ ¼å¼çš„å®Œæ•´ç»“æœ
# summary.txt          - æ–‡æœ¬æ ¼å¼çš„æ±‡æ€»
# nq_predictions.jsonl - NQ çš„è¯¦ç»†é¢„æµ‹ï¼ˆå¦‚æœä½¿ç”¨äº† --save_predictionsï¼‰
# ...

# 2. æŸ¥çœ‹æ±‡æ€»ç»“æœ
cat results/summary.txt

# 3. æŸ¥çœ‹è¯¦ç»†ç»“æœ
cat results/results.json | python -m json.tool
```

---

## ğŸ¯ å®Œæ•´å‘½ä»¤é€ŸæŸ¥è¡¨

### å¿«é€Ÿå¯åŠ¨ï¼ˆä»å…‹éš†åˆ°è¿è¡Œï¼‰

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/haowu0916edisonwu/Prior_judgement_demo.git
cd Prior_judgement_demo

# 2. åˆ›å»ºç¯å¢ƒ
conda env create -f environment.yml
conda activate priori_care

# 3. å‡†å¤‡æ•°æ®ï¼ˆå‡è®¾ä½ æœ‰ eval.zipï¼‰
unzip eval.zip -d data_care/

# 4. éªŒè¯æ•°æ®
python verify_data.py

# 5. å¿«é€Ÿæµ‹è¯•ï¼ˆ10åˆ†é’Ÿï¼‰
python run_eval.py --max_samples 10 --verbose

# 6. å®Œæ•´è¯„ä¼°ï¼ˆ1-2å°æ—¶ï¼‰
python run_eval.py

# 7. æŸ¥çœ‹ç»“æœ
cat results/summary.txt
```

---

## ğŸ› å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜ 1: æ¨¡å—å¯¼å…¥é”™è¯¯

**é”™è¯¯ä¿¡æ¯**:
```
ModuleNotFoundError: No module named 'transformers'
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿æ¿€æ´»äº†æ­£ç¡®çš„ç¯å¢ƒ
conda activate priori_care  # æˆ– source venv/bin/activate

# é‡æ–°å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

---

### é—®é¢˜ 2: CUDA ä¸å¯ç”¨

**é”™è¯¯ä¿¡æ¯**:
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ 1**: ä½¿ç”¨ CPUï¼ˆæ…¢ï¼‰
```bash
export CUDA_VISIBLE_DEVICES=""
python run_eval.py
```

**è§£å†³æ–¹æ¡ˆ 2**: æ¸…ç©º GPU ç¼“å­˜
```python
import torch
torch.cuda.empty_cache()
```

---

### é—®é¢˜ 3: æ•°æ®æ–‡ä»¶æ‰¾ä¸åˆ°

**é”™è¯¯ä¿¡æ¯**:
```
FileNotFoundError: Question file not found: data_care/eval/nq/test.jsonl
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ•°æ®ç›®å½•ç»“æ„
ls -R data_care/eval/

# ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®ä½ç½®
# åº”è¯¥çœ‹åˆ°ï¼š
# data_care/eval/nq/test.jsonl
# data_care/eval/nq/retrieval/colbertv2/test_question_aware.jsonl
```

---

### é—®é¢˜ 4: æ¨¡å‹ä¸‹è½½å¤±è´¥

**é”™è¯¯ä¿¡æ¯**:
```
ConnectionError: Can't connect to huggingface.co
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
python run_eval.py
```

---

### é—®é¢˜ 5: TriviaQA åŠ è½½å¤±è´¥

**é”™è¯¯ä¿¡æ¯**:
```
âš ï¸  Skipped 11313 samples (no matching retrieval)
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ä»£ç æ˜¯å¦æ˜¯æœ€æ–°ç‰ˆæœ¬
grep "_merge_by_query" src/data_loader.py

# å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¯´æ˜ä»£ç ä¸æ˜¯æœ€æ–°ç‰ˆ
# éœ€è¦é‡æ–°ä¸‹è½½ data_loader_ultimate.py å¹¶è¦†ç›– src/data_loader.py
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§

### æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ

```bash
# å®æ—¶ç›‘æ§ GPU
watch -n 1 nvidia-smi

# æˆ–
nvidia-smi -l 1
```

**æ­£å¸¸æ˜¾ç¤º**:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0   |
|-------------------------------+----------------------+----------------------+
|   0  NVIDIA A100-SXM... On   | 00000000:00:05.0 Off |                    0 |
| N/A   45C    P0    68W / 400W |  18234MiB / 40960MiB |     95%      Default |
+-------------------------------+----------------------+----------------------+

æ˜¾å­˜ä½¿ç”¨: 18.2 GB / 40 GB (æ­£å¸¸)
GPU åˆ©ç”¨ç‡: 95% (æ­£å¸¸)
```

---

## ğŸ¯ ä¸åŒåœºæ™¯çš„è¿è¡Œå‘½ä»¤

### åœºæ™¯ 1: è®ºæ–‡å¤ç°ï¼ˆå®Œæ•´è¯„ä¼°ï¼‰

```bash
# è¯„ä¼°æ‰€æœ‰æ•°æ®é›†ï¼Œä¿å­˜è¯¦ç»†é¢„æµ‹
python run_eval.py --save_predictions

# é¢„è®¡æ—¶é—´: 1-2 å°æ—¶ (A100)
```

---

### åœºæ™¯ 2: å¿«é€ŸéªŒè¯

```bash
# æ¯ä¸ªæ•°æ®é›†åªæµ‹è¯• 100 ä¸ªæ ·æœ¬
python run_eval.py --max_samples 100

# é¢„è®¡æ—¶é—´: 10-15 åˆ†é’Ÿ
```

---

### åœºæ™¯ 3: è°ƒè¯•ç‰¹å®šæ•°æ®é›†

```bash
# åªè¯„ä¼° NQï¼ŒæŸ¥çœ‹è¯¦ç»†è¾“å‡º
python run_eval.py --datasets nq --verbose --debug_sample

# é¢„è®¡æ—¶é—´: 5-10 åˆ†é’Ÿ
```

---

### åœºæ™¯ 4: è¯„ä¼°ç‰¹å®šæ•°æ®é›†ç»„åˆ

```bash
# åªè¯„ä¼° NQ å’Œ TriviaQA
python run_eval.py --datasets nq trivia

# é¢„è®¡æ—¶é—´: 30-40 åˆ†é’Ÿ
```

---

## ğŸ“ å®Œæ•´éƒ¨ç½²æ£€æŸ¥æ¸…å•

æ‰§è¡Œå®Œä»¥ä¸‹æ­¥éª¤ï¼Œä½ çš„é¡¹ç›®å°±å®Œå…¨æ­å»ºå¥½äº†ï¼š

- [ ] Python 3.10+ å·²å®‰è£…
- [ ] CUDA å’Œ GPU é©±åŠ¨å·²å®‰è£…ï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰
- [ ] ä»£ç å·²ä» GitHub å…‹éš†
- [ ] Python ç¯å¢ƒå·²åˆ›å»ºå¹¶æ¿€æ´»
- [ ] ä¾èµ–åŒ…å·²å®‰è£…ï¼ˆ`requirements.txt`ï¼‰
- [ ] æ•°æ®æ–‡ä»¶å·²æ”¾ç½®åœ¨æ­£ç¡®ä½ç½®
- [ ] `verify_data.py` è¿è¡ŒæˆåŠŸ
- [ ] `test_data_loading.py` è¿è¡ŒæˆåŠŸ
- [ ] å¿«é€Ÿæµ‹è¯•ï¼ˆ10ä¸ªæ ·æœ¬ï¼‰è¿è¡ŒæˆåŠŸ
- [ ] TriviaQA æ˜¾ç¤º `Match mode: QUERY`
- [ ] å‡†å¤‡å¼€å§‹å®Œæ•´è¯„ä¼°

---

## ğŸš€ ç°åœ¨å¼€å§‹è¿è¡Œï¼

```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd ~/Prior_judgement_demo

# æ¿€æ´»ç¯å¢ƒ
conda activate priori_care

# å¼€å§‹å®Œæ•´è¯„ä¼°
python run_eval.py

# æˆ–ä½¿ç”¨åå°è¿è¡Œï¼ˆæ¨èé•¿æ—¶é—´è¿è¡Œï¼‰
nohup python run_eval.py > evaluation.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f evaluation.log
```

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æ£€æŸ¥ `evaluation.log` æ—¥å¿—æ–‡ä»¶
2. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„"å¸¸è§é—®é¢˜æ’æŸ¥"éƒ¨åˆ†
3. åœ¨ GitHub ä»“åº“æ Issue
4. å‚è€ƒ README.md ä¸­çš„æ•…éšœæ’æŸ¥æŒ‡å—

---

**æœ€åæ›´æ–°**: 2024-01-06  
**æµ‹è¯•ç¯å¢ƒ**: Ubuntu 22.04, CUDA 12.0, A100 40GB  
**çŠ¶æ€**: âœ… å®Œæ•´æµ‹è¯•é€šè¿‡
