#!/usr/bin/env python3
"""
Priori Judgment ä¸“é¡¹è¯„ä¼° - TruthfulQA ç‹¬äº«ç‰ˆ (ä¿®å¤ç‰ˆ)
åªè·‘ TruthfulQAï¼Œå¹¶ç”Ÿæˆæ ‡å‡†çš„ results.json æ±‡æ€»æ–‡ä»¶
"""

import argparse
import json
import os
from pathlib import Path

# å¤ç”¨ç°æœ‰çš„æ¨¡å—
from src.data_loader import CAREDataLoader
from src.evaluator import PrioriJudgmentEvaluator
from src.metrics import Metrics

def save_predictions(results, output_dir, dataset):
    """ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœ"""
    filename = f"{dataset}_predictions.jsonl"
    if hasattr(output_dir, 'joinpath'):
        output_file = output_dir / filename
    else:
        output_file = os.path.join(output_dir, filename)

    print(f"æ­£åœ¨ä¿å­˜è¯¦ç»†é¢„æµ‹åˆ°: {output_file}")

    with open(output_file, 'w', encoding='utf-8') as f:
        for r in results:
            pred_data = {
                'id': getattr(r, 'id', 'unknown_id'),
                'question': getattr(r, 'question', ''),
                'prediction': getattr(r, 'prediction', ''),
                'gold_answers': getattr(r, 'gold_answers', []),
                'mode': getattr(r, 'mode', 'unknown'),
                'priori_output': getattr(r, 'priori_output', None),
            }
            pred_data['correct'] = False 
            f.write(json.dumps(pred_data, ensure_ascii=False) + '\n')
    
    print("ä¿å­˜å®Œæˆï¼")

def main():
    parser = argparse.ArgumentParser(description="Priori Judgment - TruthfulQA Only")
    parser.add_argument("--data_root", default="data_care/eval", help="Data directory")
    parser.add_argument("--model_name", default="NousResearch/Meta-Llama-3-8B-Instruct", help="Model name")
    parser.add_argument("--output_dir", default="results_truthfulqa_v24", help="Output directory") 
    parser.add_argument("--max_samples", type=int, default=None, help="Limit samples (debug)")
    parser.add_argument("--save_predictions", action="store_true", default=True, help="Save detailed predictions") 
    args = parser.parse_args()
    
    dataset = 'truthfulqa'
    targets = {'f1': 0.2540, 'rouge_l': 0.2310}
    
    print("=" * 70)
    print("ğŸš€ Priori Judgment Evaluation - TRUTHFULQA ONLY (v24.0 Check)")
    print("=" * 70)
    print(f"Model: {args.model_name}")
    print(f"Output: {args.output_dir}")
    print("=" * 70)
    
    # åˆå§‹åŒ–
    data_loader = CAREDataLoader(args.data_root, verbose=False)
    evaluator = PrioriJudgmentEvaluator(args.model_name)
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š Evaluating {dataset.upper()}")
    print(f"{'='*70}")
    
    # åŠ è½½æ•°æ®
    samples = data_loader.load_dataset(dataset)
    if args.max_samples:
        samples = samples[:args.max_samples]
        print(f"ğŸ”§ Debug mode: using {len(samples)} samples")
    
    # è¯„ä¼°
    result = evaluator.evaluate_dataset(samples)
    metrics = result['metrics']
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # 1. ä¿å­˜è¯¦ç»†é¢„æµ‹ (.jsonl)
    if args.save_predictions:
        save_predictions(result['results'], output_dir, dataset)
    
    # 2. [å…³é”®ä¿®å¤] ä¿å­˜æ ‡å‡†æ±‡æ€»ç»“æœ (results.json)
    # è¿™å°±æ˜¯æ‚¨è¦çš„é‚£ä¸ªæ ¼å¼
    final_results = {
        dataset: {
            "metrics": metrics,
            "mode_distribution": result['mode_distribution']
        }
    }
    
    json_path = output_dir / "results.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°ç»“æœ
    print(f"\nğŸ“ˆ Results (TruthfulQA):")
    f1 = metrics.get('f1', 0)
    rouge = metrics.get('rouge_l', 0)
    f1_status = "âœ…" if f1 >= targets['f1'] - 0.005 else "âŒ"
    rouge_status = "âœ…" if rouge >= targets['rouge_l'] - 0.005 else "âŒ"
    
    print(f"  F1:      {f1:.4f} (Target: {targets['f1']:.4f}) {f1_status}")
    print(f"  ROUGE-L: {rouge:.4f} (Target: {targets['rouge_l']:.4f}) {rouge_status}")
    
    print(f"\nğŸ”€ Mode Distribution:")
    for mode, count in result['mode_distribution'].items():
        pct = count / len(samples) * 100
        print(f"  {mode}: {count}/{len(samples)} ({pct:.1f}%)")

    print(f"\nğŸ’¾ Files saved:")
    print(f"  - {json_path} (æ ‡å‡† JSON æ±‡æ€»)")
    print(f"  - {output_dir}/{dataset}_predictions.jsonl (è¯¦ç»†é¢„æµ‹)")

if __name__ == "__main__":
    main()