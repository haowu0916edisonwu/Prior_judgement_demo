#!/usr/bin/env python3
"""
æé€Ÿç®—åˆ†è„šæœ¬ (Fast Re-Evaluation)
ä¸éœ€è¦åŠ è½½æ¨¡å‹ï¼Œç›´æ¥è¯»å–ç°æœ‰çš„ .jsonl æ–‡ä»¶ï¼ŒæŒ‰ç…§è®ºæ–‡æ ‡å‡† (åˆ†æ¯=6) é‡æ–°è®¡ç®—å¹³å‡åˆ†ã€‚
"""

import json
import os
import argparse
from pathlib import Path
import numpy as np
from src.metrics import Metrics  # å¤ç”¨ä½ ç°æœ‰çš„æŒ‡æ ‡è®¡ç®—é€»è¾‘

def load_jsonl(file_path):
    data = []
    if not os.path.exists(file_path):
        print(f"âš ï¸ Warning: File not found: {file_path}")
        return []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--results_dir", default="results", help="Directory containing .jsonl predictions")
    parser.add_argument("--output_file", default="final_paper_results.json", help="Output JSON file")
    args = parser.parse_args()

    results_dir = Path(args.results_dir)
    
    # è®ºæ–‡å¯¹é½çš„è¯„ä¼°é…ç½®
    # (æ•°æ®é›†åç§°, JSONLæ–‡ä»¶å, æŒ‡æ ‡å‡½æ•°, æŒ‡æ ‡åç§°, ç›®æ ‡åˆ†æ•°)
    tasks = [
        ('nq', 'nq_predictions.jsonl', Metrics.compute_span_em, 'span_em', 0.458),
        ('trivia', 'trivia_predictions.jsonl', Metrics.compute_span_em, 'span_em', 0.704),
        ('webqa', 'webqa_predictions.jsonl', Metrics.compute_span_em, 'span_em', 0.406),
        ('truthfulqa', 'truthfulqa_predictions.jsonl', Metrics.compute_f1, 'f1', 0.254),
        ('truthfulqa', 'truthfulqa_predictions.jsonl', Metrics.compute_rouge_l, 'rouge_l', 0.231), # åŒä¸€ä»½æ–‡ä»¶ç®—ä¸¤æ¬¡
        ('factkg', 'factkg_predictions.jsonl', Metrics.compute_accuracy, 'accuracy', 0.666)
    ]

    final_report = {}
    collected_scores = []

    print(f"{'='*70}")
    print(f"ğŸš€ FAST RE-EVALUATION (Paper Aligned: Average / 6)")
    print(f"ğŸ“‚ Reading from: {results_dir}")
    print(f"{'='*70}")

    # ç¼“å­˜å·²è¯»å–çš„æ•°æ®ï¼Œé¿å… TruthfulQA è¯»ä¸¤æ¬¡
    data_cache = {}

    for dataset, filename, metric_fn, metric_name, target in tasks:
        file_path = results_dir / filename
        
        # 1. åŠ è½½æ•°æ® (å¦‚æœæ²¡åŠ è½½è¿‡)
        if filename not in data_cache:
            data_cache[filename] = load_jsonl(file_path)
        
        samples = data_cache[filename]
        if not samples:
            print(f"âŒ Skipping {dataset} ({metric_name}): No data found.")
            continue

        # 2. é‡æ–°è®¡ç®—åˆ†æ•° (Re-compute)
        # æˆ‘ä»¬é‡æ–°è·‘ä¸€é metric_fnï¼Œç¡®ä¿é€»è¾‘ä¸ç°åœ¨çš„ä¸€è‡´
        scores = []
        for item in samples:
            pred = item['prediction']
            golds = item['gold_answers']
            scores.append(metric_fn(pred, golds))
        
        avg_score = sum(scores) / len(scores) if scores else 0
        
        # 3. è®°å½•
        collected_scores.append(avg_score)
        
        if dataset not in final_report:
            final_report[dataset] = {}
        final_report[dataset][metric_name] = avg_score

        # 4. æ‰“å°çŠ¶æ€
        status = "âœ…" if avg_score >= target - 0.005 else "âŒ"
        print(f"{dataset:<15} {metric_name.upper():<10} {avg_score:.4f} (Target: {target:.4f}) {status}")

    # 5. è®¡ç®—æœ€ç»ˆå¹³å‡åˆ†
    print(f"{'-'*70}")
    if len(collected_scores) == 6:
        final_avg = sum(collected_scores) / 6
        target_avg = 0.453
        status = "âœ…" if final_avg >= target_avg - 0.005 else "âŒ"
        print(f"{'AVERAGE (Div/6)':<26} {final_avg:.4f} (Target: {target_avg:.4f}) {status}")
        
        final_report['AVERAGE'] = final_avg
    else:
        print(f"âš ï¸ Warning: Only found {len(collected_scores)}/6 components. Cannot compute valid average.")

    print(f"{'='*70}")

    # ä¿å­˜ç»“æœ
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, indent=2)
    print(f"ğŸ’¾ Report saved to {args.output_file}")

if __name__ == "__main__":
    main()