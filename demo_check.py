#!/usr/bin/env python3
"""
Demo Check - å¿«é€Ÿæ£€æŸ¥æ¨¡å‹è¾“å‡ºå’Œæ¸…æ´—æ•ˆæœ
(Strictly aligned with your Git Repo logic)
"""

import argparse
import random
import sys
import os
import torch

# ç¡®ä¿èƒ½å¯¼å…¥ src ç›®å½•ä¸‹çš„æ¨¡å— (å’Œ run_eval.py ä¸€è‡´)
sys.path.append(os.getcwd())

from src.data_loader import CAREDataLoader
from src.evaluator import PrioriJudgmentEvaluator

def parse_args():
    parser = argparse.ArgumentParser(description="Demo Check for Priori Judgment System")
    
    # === æ ¸å¿ƒå‚æ•° (å®Œå…¨å¯¹é½ run_eval.py çš„å‚æ•°å‘½å) ===
    parser.add_argument("--model_name", type=str, 
                        default="NousResearch/Meta-Llama-3-8B-Instruct", 
                        help="Model name or path")
    
    parser.add_argument("--data_root", type=str, 
                        default="data_care/eval", 
                        help="Data directory")
    
    # === Check ä¸“ç”¨å‚æ•° ===
    # é»˜è®¤æ£€æŸ¥è¿™ä¸‰ä¸ªå…³é”®æ•°æ®é›†
    parser.add_argument("--datasets", nargs='+', 
                        default=["nq", "trivia", "webqa"], 
                        help="Datasets to check")
    
    parser.add_argument("--num_samples", type=int, default=2, 
                        help="Number of random samples to check per dataset")
    
    # å¢åŠ  device å‚æ•°æ–¹ä¾¿è°ƒè¯•
    parser.add_argument("--device", type=str, 
                        default="cuda" if torch.cuda.is_available() else "cpu",
                        help="Device to run the model on")
    
    return parser.parse_args()

def run_demo(args):
    print(f"\nğŸ”§ [Init] Loading Model from: {args.model_name}")
    print(f"   (Device: {args.device})")
    
    try:
        # å¤ç”¨ä½  evaluator.py çš„åˆå§‹åŒ–é€»è¾‘
        evaluator = PrioriJudgmentEvaluator(args.model_name, device=args.device)
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·æ£€æŸ¥è·¯å¾„ '{args.model_name}' æ˜¯å¦æ­£ç¡®ï¼Ÿ")
        return

    print(f"\nğŸ“‚ [Init] Loading Data Loader from: {args.data_root}")
    try:
        # å¤ç”¨ä½  data_loader.py çš„åˆå§‹åŒ–é€»è¾‘
        loader = CAREDataLoader(args.data_root)
    except Exception as e:
        print(f"\nâŒ æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·æ£€æŸ¥ data_root '{args.data_root}' æ˜¯å¦å­˜åœ¨ï¼Ÿ")
        return

    print("\n" + "="*70)
    print(f"ğŸ” SOTA Logic & Prompt Check (Running {args.num_samples} samples per dataset)")
    print("="*70)

    for ds_name in args.datasets:
        print(f"\nğŸ“‚ Dataset: {ds_name.upper()}")
        try:
            # load_dataset ä¼šè‡ªåŠ¨å¤„ç† trivia -> triviaqa çš„æ˜ å°„
            samples = loader.load_dataset(ds_name)
        except Exception as e:
            print(f"   âš ï¸ Skipping {ds_name}: {e}")
            continue
        
        if not samples:
            print(f"   âš ï¸ No samples found for {ds_name}")
            continue

        # éšæœºé‡‡æ ·
        k = min(len(samples), args.num_samples)
        test_samples = random.sample(samples, k)
        
        for i, sample in enumerate(test_samples, 1):
            print(f"\nğŸ”¹ [Sample #{i} | ID: {sample.id}]")
            print(f"   Q: {sample.question}")
            
            # === è¿è¡Œè¯„ä¼° (è°ƒç”¨ä½ ä¿®æ”¹åçš„ evaluator.py) ===
            result = evaluator.evaluate_sample(sample)
            
            # === ç»“æœåˆ†æ ===
            raw_out = result.priori_output
            clean_pred = result.prediction
            mode = result.mode
            
            # 1. æ£€æŸ¥ Prompt é”šç‚¹æ˜¯å¦ç”Ÿæ•ˆ
            # å¦‚æœ prompts.py æ”¹å¥½äº†ï¼Œraw_out åº”è¯¥ç›´æ¥æ˜¯ç­”æ¡ˆï¼Œä¸åŒ…å« '?\nAnswer:'
            prompt_status = "âœ… Clean"
            if "?\nAnswer:" in raw_out:
                prompt_status = "âŒ LEAKED (Old Prompt detected)"
            elif raw_out.strip().startswith("Note:") or raw_out.strip().startswith("Question:"):
                 prompt_status = "âš ï¸ Messy Start"

            print(f"   --------------------------------------------------")
            print(f"   ğŸ¤– Raw Output   : {repr(raw_out)}")
            print(f"   ğŸ” Prompt Status: {prompt_status}")
            print(f"   âœ¨ Final Answer : {repr(clean_pred)}")
            print(f"   ğŸ·ï¸  Mode        : {mode.upper()}")
            
            # 2. ç­–ç•¥é€»è¾‘éªŒè¯ (Strategy Check)
            
            # Check TriviaQA: åº”è¯¥å…è®¸ Unknown -> Closedbook
            if ds_name in ['trivia', 'triviaqa']:
                if "unknown" in clean_pred.lower():
                    if mode == 'closedbook':
                        print(f"   âœ… Strategy: Correctly fell back to Closed-book")
                    else:
                        print(f"   âŒ Strategy: Predicted Unknown but stuck in RAG (Strategy B1 failed?)")
            
            # Check NQ/WebQA: åº”è¯¥æ•æ‰ 'does not mention'
            if ds_name in ['nq', 'webqa']:
                 refusal_keywords = ["does not mention", "not provided", "no information", "cannot answer"]
                 is_refusal = any(k in raw_out.lower() for k in refusal_keywords)
                 
                 if is_refusal:
                     if mode == 'closedbook':
                         print(f"   âœ… Strategy: Caught indirect refusal ('{clean_pred[:20]}...') -> Closedbook")
                     else:
                         # æ³¨æ„ï¼šå¦‚æœä½ ä¿ç•™äº†â€œå¼ºè¡Œå¬å›â€ç­–ç•¥ï¼Œè¿™é‡Œå¯èƒ½æ˜¯ RAGï¼Œè§†ä½ çš„ä¿®æ”¹è€Œå®š
                         print(f"   â„¹ï¸  Strategy: Refusal detected but kept in RAG (Check if this is intended)")
            
            print(f"   --------------------------------------------------")

    print("\nâœ… Check finished!")

if __name__ == "__main__":
    args = parse_args()
    run_demo(args)