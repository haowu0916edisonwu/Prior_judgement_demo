"""
Experimental Patch: Soft-Refusal Rescue (Ablation Study)
------------------------------------------------------------------
å®éªŒç›®çš„ (Objective):
æ¢ç©¶ NQ å’Œ TriviaQA æ•°æ®é›†ä¸­æ˜¯å¦å­˜åœ¨â€œä¼ªæ‹’ç­”ï¼ˆPseudo-Refusalsï¼‰â€å¯¼è‡´çš„æ€§èƒ½æŸå¤±ã€‚
å³ï¼šæ¨¡å‹è¾“å‡ºäº† "Context does not mention..." ä½†æœªè§¦å‘é—­å·æ¨¡å¼çš„æƒ…å†µã€‚

æ–¹æ³• (Methodology):
1. æ‰«æ RAG è¾“å‡ºï¼Œæå–åŒ…å« "not mention", "no information" ç­‰æ‹’ç­”å…³é”®è¯çš„æ ·æœ¬ã€‚
2. å¼ºåˆ¶å°†è¿™äº›æ ·æœ¬çš„æ¨¡å¼ä» 'rag' åˆ‡æ¢ä¸º 'closedbook'ã€‚
3. ä½¿ç”¨ Llama-3 æ¨¡å‹é‡æ–°ç”Ÿæˆç­”æ¡ˆã€‚

å®éªŒç»“è®º (Conclusion):
ç»è¿‡ v26/v27 ä¸¤è½®æµ‹è¯•ï¼Œå‘ç°åˆ†æ•°å¹¶æœªæ˜¾è‘—æå‡ï¼ˆNQ ä¿æŒåœ¨ 0.4457ï¼ŒTriviaQA ä¿æŒåœ¨ 0.7034ï¼‰ã€‚
åŸå› åˆ†æï¼š
1. æ¨¡å‹æœ¬èº«çš„ Priori Judgment (v25) å·²ç»è¾¾åˆ°äº†å¸•ç´¯æ‰˜æœ€ä¼˜ï¼Œå¤§å¤šæ•°â€œæ‹’ç­”â€æ˜¯ç”±äºæ¨¡å‹ç¡®å®ä¸çŸ¥é“ç­”æ¡ˆï¼ˆKnowledge Gapï¼‰ï¼Œåˆ‡æ¢é—­å·åä¾ç„¶æ— æ³•å›ç­”ã€‚
2. å¼ºåˆ¶åˆ‡æ¢é—­å·å¼•å…¥äº† False Positive é£é™©ï¼ˆè¯¯ä¼¤äº†åŒ…å« "unknown" å•è¯çš„æ­£ç¡®ç­”æ¡ˆï¼‰ã€‚

æœ€ç»ˆå†³ç­– (Verdict):
ä¿ç•™ v25.0 ç‰ˆæœ¬ç»“æœä½œä¸ºæœ€ç»ˆ Baselineã€‚æœ¬è„šæœ¬ç•™ä½œå®éªŒè®°å½•ï¼Œè¯æ˜æˆ‘ä»¬
"""
import json
import torch
import os
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- é…ç½® ---
MODEL_NAME = "NousResearch/Meta-Llama-3-8B-Instruct"
DATA_DIR = "results"         # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è¦è¯»å– v25 (åŸå§‹ SOTA) çš„æ•°æ®ï¼Œä¸è¦è¯» v26 (å·²æŸå) çš„ï¼
OUTPUT_DIR = "results_v27"   # å­˜åˆ°æ–°ç›®å½•

# 1. ç»å¯¹å®‰å…¨çš„ Trigger (å»æ‰ unknown, unclear)
SAFE_TRIGGERS = [
    "does not mention", "doesn't mention", 
    "not provide", "no information",
    "context does not", "passage does not",
    "cannot answer", "text does not",
    "provided text"
]

TARGETS = [
    {"name": "nq", "filename": "nq_predictions.jsonl"},
    {"name": "trivia", "filename": "trivia_predictions.jsonl"}
]

def load_jsonl(path):
    data = []
    if os.path.exists(path):
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line))
    return data

def save_jsonl(data, path):
    with open(path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

def main():
    print(f"ğŸš‘ å¯åŠ¨ v27.0 æœ€ç»ˆå¾®åˆ›æ‰‹æœ¯ (NQ + Trivia)...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    tasks = []
    
    # 1. æ‰«æä¸ç­›é€‰
    for t in TARGETS:
        path = os.path.join(DATA_DIR, t["filename"])
        print(f"ğŸ“– è¯»å–åŸå§‹ v25 æ•°æ®: {t['name']}...")
        
        if not os.path.exists(path):
            print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {path}ï¼Œè¯·ç¡®ä¿ results æ–‡ä»¶å¤¹é‡Œæœ‰åŸå§‹æ•°æ®ï¼")
            continue
            
        data = load_jsonl(path)
        indices_to_rescue = []
        skipped_long = 0
        skipped_ambiguous = 0
        
        for i, item in enumerate(data):
            if item.get('mode') == 'rag':
                pred = item.get('prediction', '').strip()
                pred_lower = pred.lower()
                
                # A. å‘½ä¸­ä¸¥æ ¼æ‹’ç­”è¯
                hit_trigger = any(trig in pred_lower for trig in SAFE_TRIGGERS)
                
                # B. é•¿åº¦æ£€æŸ¥ (å…³é”®ï¼)
                # å¦‚æœå›ç­”å¾ˆé•¿(>20è¯)ï¼Œå¤§æ¦‚ç‡æ˜¯æ­£ç¡®çš„è§£é‡Šï¼Œæˆ–è€…æ˜¯ "Although unknown, it is X"
                is_short = len(pred.split()) < 20
                
                if hit_trigger:
                    if is_short:
                        indices_to_rescue.append(i)
                    else:
                        skipped_long += 1
                elif "unknown" in pred_lower:
                    # è®°å½•ä¸€ä¸‹æˆ‘ä»¬æ•…æ„æ”¾è¿‡çš„ unknown
                    skipped_ambiguous += 1

        print(f"   [{t['name'].upper()}] æ‰«æç»“æœ:")
        print(f"     - ğŸš‘ å¾…æ‰‹æœ¯ (ç¡®ä¿¡æ‹’ç­”): {len(indices_to_rescue)}")
        print(f"     - ğŸ›¡ï¸ å·²ä¿æŠ¤ (é•¿å¥è¯¯ä¼¤): {skipped_long} (v26å°±æ˜¯æ­»åœ¨è¿™é‡Œ)")
        print(f"     - ğŸ›¡ï¸ å·²å¿½ç•¥ (Unknown): {skipped_ambiguous}")
        
        if indices_to_rescue:
            tasks.append({'name': t['name'], 'data': data, 'indices': indices_to_rescue, 'filename': t['filename']})
        else:
            # å¦‚æœæ²¡å¾—æ•‘ï¼Œç›´æ¥ä¿å­˜åŸç‰ˆ
            save_jsonl(data, os.path.join(OUTPUT_DIR, t["filename"]))
            print(f"     âœ… æ— éœ€ä¿®å¤ï¼Œå·²åŸæ ·ä¿å­˜ã€‚")

    if not tasks:
        print("âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•ã€‚")
        return

    # 2. åŠ è½½æ¨¡å‹
    print(f"\nğŸ”§ åŠ è½½æ¨¡å‹ {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto")
    model.eval()

    # 3. æ‰§è¡Œæ‰‹æœ¯
    print(f"\nâš¡ å¼€å§‹å¾®åˆ›æ‰‹æœ¯...")
    
    def make_prompt(q):
        return f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nAnswer the question concisely.\nQuestion: {q}\nAnswer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

    for task in tasks:
        name = task['name']
        data = task['data']
        indices = task['indices']
        
        print(f"ğŸ‘‰ ä¿®å¤ {name} ({len(indices)} é¢˜)...")
        
        for idx in tqdm(indices):
            item = data[idx]
            q = item['question']
            
            # ç”Ÿæˆ
            inputs = tokenizer(make_prompt(q), return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs, max_new_tokens=20, do_sample=False,
                    pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id
                )
            new_pred = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
            
            # æˆªæ–­
            if "." in new_pred: new_pred = new_pred.split(".", 1)[0].strip()
            
            # æ›´æ–°
            item['prediction'] = new_pred
            item['mode'] = 'closedbook_rescue'
            data[idx] = item
            
        # ä¿å­˜
        out_path = os.path.join(OUTPUT_DIR, task['filename'])
        save_jsonl(data, out_path)
        print(f"âœ… {name} ä¿®å¤ç‰ˆå·²ä¿å­˜")

    # 4. å¤åˆ¶å…¶ä»– SOTA æ–‡ä»¶ (WebQA, FactKG, TruthfulQA)
    import shutil
    other_files = ["webqa_predictions.jsonl", "truthfulqa_predictions.jsonl", "factkg_predictions.jsonl"]
    for f_name in other_files:
        src = os.path.join(DATA_DIR, f_name)
        dst = os.path.join(OUTPUT_DIR, f_name)
        if os.path.exists(src):
            shutil.copy(src, dst)
            print(f"ğŸ“¦ å·²å¤åˆ¶ {f_name}")

    print(f"\nğŸ‰ v27.0 ç»“æŸï¼è¯·è¿è¡Œ: python fast_re_eval.py --results_dir results_v27")

if __name__ == "__main__":
    main()