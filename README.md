# Priori Judgment Baseline for RAG Evaluation

[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![PyTorch 2.1](https://img.shields.io/badge/PyTorch-2.1-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

å®Œæ•´å¤ç° **EMNLP 2025 CARE** è®ºæ–‡ Table 2 ä¸­çš„ **Priori Judgment** åŸºå‡†æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨å¤„ç†çŸ¥è¯†å†²çªæ—¶çš„æ€§èƒ½ã€‚

---

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [æ€§èƒ½ç›®æ ‡](#æ€§èƒ½ç›®æ ‡)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [è¾“å‡ºè¯´æ˜](#è¾“å‡ºè¯´æ˜)
- [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)
- [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
- [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)

---

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº† **Priori Judgment** åŸºå‡†æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä¸¤é˜¶æ®µæ¨ç†ç­–ç•¥ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœºæ™¯ä¸‹å¤„ç†çŸ¥è¯†å†²çªçš„èƒ½åŠ›ã€‚

### ä»€ä¹ˆæ˜¯ Priori Judgmentï¼Ÿ

Priori Judgment æ˜¯ä¸€ç§å…ˆåˆ¤æ–­åå›ç­”çš„ç­–ç•¥ï¼š

1. **Stage 1 - å…ˆéªŒåˆ¤æ–­ï¼ˆPriori Judgmentï¼‰**ï¼š
   - æ¨¡å‹é¦–å…ˆè¯„ä¼°æ˜¯å¦èƒ½åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æˆ–å†…éƒ¨çŸ¥è¯†å›ç­”é—®é¢˜
   - å¦‚æœå¯ä»¥å›ç­” â†’ ç»™å‡ºç®€çŸ­ç­”æ¡ˆ
   - å¦‚æœä¸èƒ½å›ç­” â†’ è¾“å‡º "Unknown"

2. **Stage 2 - é—­å·å›é€€ï¼ˆClosed-book Fallbackï¼‰**ï¼š
   - å¦‚æœ Stage 1 è¾“å‡º "Unknown"
   - å›é€€åˆ°çº¯é—­å·æ¨¡å¼ï¼ˆä¸ä½¿ç”¨æ£€ç´¢ç»“æœï¼‰
   - ä»…ä¾èµ–æ¨¡å‹çš„å‚æ•°åŒ–çŸ¥è¯†å›ç­”

è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹åœ¨æ£€ç´¢ç»“æœä¸å¯é æ—¶ï¼Œä¸»åŠ¨é€‰æ‹©ä¾èµ–è‡ªèº«çŸ¥è¯†ï¼Œä»è€Œæé«˜åœ¨çŸ¥è¯†å†²çªåœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚

---

## ğŸ“Š æ€§èƒ½ç›®æ ‡

æœ¬å®ç°æ—¨åœ¨å¤ç°ä»¥ä¸‹æ€§èƒ½æŒ‡æ ‡ï¼ˆLLaMA-3-8B-Instruct on CARE Datasetsï¼‰ï¼š

| Dataset | Task Type | Metric | Target | è¯´æ˜ |
|---------|-----------|--------|--------|------|
| **Natural Questions** | Open-domain QA | Span EM | **0.458** | äº‹å®æ€§é—®ç­” |
| **TriviaQA** | Open-domain QA | Span EM | **0.704** | çäº‹é—®ç­” |
| **WebQuestions** | Open-domain QA | Span EM | **0.406** | Webæœç´¢é—®ç­” |
| **TruthfulQA** | Long-form QA | F1 | **0.254** | çœŸå®æ€§é—®ç­” |
| **TruthfulQA** | Long-form QA | ROUGE-L | **0.231** | ç”Ÿæˆè´¨é‡ |
| **FactKG** | Fact Verification | Accuracy | **0.666** | äº‹å®éªŒè¯ |
| **Average** | - | - | **0.453** | ç»¼åˆæ€§èƒ½ |

> ğŸ“Œ **æ³¨æ„**ï¼šè¿™äº›æŒ‡æ ‡æ¥è‡ª EMNLP 2025 CARE è®ºæ–‡ Table 2ï¼Œä½¿ç”¨ ColBERTv2 question-aware retrievalã€‚

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### 1. **å®Œæ•´å¤ç°è®ºæ–‡è®¾ç½®**
- âœ… ä¸¥æ ¼æŒ‰ç…§ COLING 2025 Table 5 & 6 çš„ Prompt æ¨¡æ¿
- âœ… ä½¿ç”¨ ColBERTv2 question-aware æ£€ç´¢ï¼ˆTop-1ï¼‰
- âœ… ä¸¤é˜¶æ®µæ¨ç†é€»è¾‘ï¼ˆPriori â†’ Fallbackï¼‰
- âœ… é€‚é… CARE å®é™…æ•°æ®æ ¼å¼

### 2. **å¤šæ•°æ®é›†è¯„ä¼°**
- âœ… 5 ä¸ªæ•°æ®é›†ï¼šNQ, TriviaQA, WebQA, TruthfulQA, FactKG
- âœ… 3 ç§ä»»åŠ¡ç±»å‹ï¼šOpen-domain QA, Long-form QA, Fact Verification
- âœ… 4 ç§è¯„ä¼°æŒ‡æ ‡ï¼šSpan EM, F1, ROUGE-L, Accuracy

### 3. **å¼€ç®±å³ç”¨**
- âœ… ä½¿ç”¨ç¤¾åŒºæ¨¡å‹ï¼ˆæ— éœ€ç”³è¯· Meta æƒé™ï¼‰
- âœ… å®Œæ•´çš„ç¯å¢ƒé…ç½®å’Œä¾èµ–ç®¡ç†
- âœ… è¯¦ç»†çš„æ•°æ®åŠ è½½å’ŒéªŒè¯è„šæœ¬

### 4. **è°ƒè¯•å‹å¥½**
- âœ… å•æ ·æœ¬è°ƒè¯•æ¨¡å¼
- âœ… è¯¦ç»†çš„ä¸­é—´è¾“å‡º
- âœ… æ¨¡å¼åˆ†å¸ƒç»Ÿè®¡ï¼ˆRAG vs Closed-bookï¼‰

---

## ğŸ”§ ç¯å¢ƒé…ç½®

### ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Linux / macOS / Windows (WSL)
- **Python**: 3.10+
- **GPU**: NVIDIA GPU with 16GB+ VRAM (æ¨è)
- **CPU**: å¯é€‰ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢

### æ–¹æ³• 1ï¼šä½¿ç”¨ Condaï¼ˆæ¨èï¼‰

```bash
# 1. åˆ›å»ºç¯å¢ƒ
conda env create -f environment.yml

# 2. æ¿€æ´»ç¯å¢ƒ
conda activate priori_care

# 3. éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
```

### æ–¹æ³• 2ï¼šä½¿ç”¨ pip

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. éªŒè¯å®‰è£…
python -c "import torch; print(torch.__version__)"
```

### ä¾èµ–åŒ…æ¸…å•

| åŒ…å | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|
| `torch` | 2.1.2 | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| `transformers` | 4.36.2 | LLM æ¨ç† |
| `accelerate` | 0.25.0 | æ¨¡å‹åŠ è½½ä¼˜åŒ– |
| `rouge-score` | 0.1.2 | ROUGE æŒ‡æ ‡ |
| `tqdm` | 4.66.1 | è¿›åº¦æ¡ |

---

## ğŸ“ æ•°æ®å‡†å¤‡

### æ•°æ®ç»“æ„è¦æ±‚

ç¡®ä¿ä½ çš„æ•°æ®ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```
data_care/eval/
â”œâ”€â”€ nq/
â”‚   â”œâ”€â”€ test.jsonl                                    # é—®é¢˜+ç­”æ¡ˆ
â”‚   â””â”€â”€ retrieval/colbertv2/
â”‚       â”œâ”€â”€ test.jsonl                                # æ ‡å‡†æ£€ç´¢
â”‚       â””â”€â”€ test_question_aware.jsonl                 # âœ… ä½¿ç”¨è¿™ä¸ª
â”œâ”€â”€ triviaqa/
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â””â”€â”€ retrieval/colbertv2/
â”‚       â”œâ”€â”€ test.jsonl
â”‚       â””â”€â”€ test_question_aware.jsonl                 # âœ… ä½¿ç”¨è¿™ä¸ª
â”œâ”€â”€ webqa/
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â””â”€â”€ retrieval/colbertv2/test_question_aware.jsonl # âœ… ä½¿ç”¨è¿™ä¸ª
â”œâ”€â”€ truthfulqa/
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â””â”€â”€ retrieval/colbertv2/test_question_aware.jsonl # âœ… ä½¿ç”¨è¿™ä¸ª
â””â”€â”€ factkg/
    â”œâ”€â”€ test.jsonl
    â””â”€â”€ retrieval/colbertv2/test_question_aware.jsonl # âœ… ä½¿ç”¨è¿™ä¸ª
```

> ğŸ“Œ **å…³é”®**ï¼šæœ¬é¡¹ç›®ä½¿ç”¨ `test_question_aware.jsonl`ï¼ˆè®ºæ–‡è®¾ç½®ï¼‰ï¼Œè€Œé `test.jsonl`

### æ•°æ®æ ¼å¼è¯´æ˜

#### 1. é—®é¢˜æ–‡ä»¶æ ¼å¼ï¼ˆ`test.jsonl`ï¼‰

```json
{
  "id": 0,
  "question": "when was the last time anyone was on the moon",
  "answer": ["14 December 1972 UTC", "December 1972"],
  "entity": "14 December 1972 UTC"
}
```

**å­—æ®µè¯´æ˜**ï¼š
- `id`: æ ·æœ¬å”¯ä¸€æ ‡è¯†ç¬¦
- `question`: é—®é¢˜æ–‡æœ¬ï¼ˆFactKG ä¸­ä¸º `claim`ï¼‰
- `answer`: æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨ï¼ˆæ³¨æ„æ˜¯ `answer` ä¸æ˜¯ `answers`ï¼‰
- `entity`: ä¸»å®ä½“ï¼ˆå¯é€‰ï¼‰

#### 2. æ£€ç´¢æ–‡ä»¶æ ¼å¼ï¼ˆ`test_question_aware.jsonl`ï¼‰

```json
{
  "id": 0,
  "query": "when was the last time anyone was on the moon",
  "topk": [
    {
      "text": "Question: when was the last time anyone was on the moon\n Document: Space technology | ... December 1972 ..."
    }
  ]
}
```

**å­—æ®µè¯´æ˜**ï¼š
- `id`: ä¸é—®é¢˜æ–‡ä»¶å¯¹åº”çš„ID
- `query`: æŸ¥è¯¢æ–‡æœ¬
- `topk`: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆæ³¨æ„æ˜¯ `topk` ä¸æ˜¯ `ctxs`ï¼‰
- `text`: åŒ…å« `Question: ... Document: ...` æ ¼å¼ï¼Œä»£ç ä¼šè‡ªåŠ¨æå– Document éƒ¨åˆ†

### éªŒè¯æ•°æ®

è¿è¡ŒéªŒè¯è„šæœ¬ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼š

```bash
# éªŒè¯æ•°æ®ç»“æ„
python verify_data.py

# æµ‹è¯•æ•°æ®åŠ è½½
python test_data_loading.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… Test Passed! Data format understood correctly.
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®Œæ•´è¯„ä¼°ï¼ˆæ¨èï¼‰

è¯„ä¼°æ‰€æœ‰ 5 ä¸ªæ•°æ®é›†ï¼š

```bash
# ä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰
./run.sh

# æˆ–æ‰‹åŠ¨è¿è¡Œ
python run_eval.py \
    --data_root data_care/eval \
    --model_name NousResearch/Meta-Llama-3-8B-Instruct \
    --output_dir results
```

**é¢„è®¡æ—¶é—´**ï¼š
- GPU (V100): ~2-3 å°æ—¶
- GPU (A100): ~1-2 å°æ—¶
- CPU: ~10-15 å°æ—¶

### 2. è°ƒè¯•æ¨¡å¼

æµ‹è¯•å•ä¸ªæ ·æœ¬ä»¥éªŒè¯é€»è¾‘ï¼š

```bash
python run_eval.py --debug_sample --datasets nq
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ› DEBUG MODE: NQ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ Sample Info:
  ID: 0
  Question: when was the last time anyone was on the moon
  Answers: ['14 December 1972 UTC', 'December 1972']
  Context length: 704 chars

ğŸ”„ Running Two-Stage Inference...

ğŸ“Š Results:
  Stage 1 (Priori) Output: December 1972
  Unknown detected: False
  Final Answer: December 1972
  Mode: rag
  Gold Answers: ['14 December 1972 UTC', 'December 1972']

âœ… Span EM: 1.0000
```

### 3. å°è§„æ¨¡æµ‹è¯•

å¿«é€ŸéªŒè¯æµç¨‹ï¼ˆæ¯ä¸ªæ•°æ®é›† 100 ä¸ªæ ·æœ¬ï¼‰ï¼š

```bash
python run_eval.py --max_samples 100
```

### 4. ç‰¹å®šæ•°æ®é›†è¯„ä¼°

åªè¯„ä¼°æŸäº›æ•°æ®é›†ï¼š

```bash
# è¯„ä¼° NQ å’Œ TriviaQA
python run_eval.py --datasets nq trivia

# è¯„ä¼° TruthfulQAï¼ˆé•¿æ–‡æœ¬ï¼‰
python run_eval.py --datasets truthfulqa --save_predictions
```

---

## ğŸ“– ä½¿ç”¨æŒ‡å—

### å‘½ä»¤è¡Œå‚æ•°

```bash
python run_eval.py [OPTIONS]
```

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--data_root` | str | `data_care/eval` | æ•°æ®æ ¹ç›®å½• |
| `--model_name` | str | `NousResearch/Meta-Llama-3-8B-Instruct` | æ¨¡å‹åç§° |
| `--output_dir` | str | `results` | ç»“æœè¾“å‡ºç›®å½• |
| `--datasets` | list | `None` (å…¨éƒ¨) | æŒ‡å®šè¯„ä¼°çš„æ•°æ®é›† |
| `--max_samples` | int | `None` (å…¨éƒ¨) | é™åˆ¶æ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬æ•° |
| `--debug_sample` | flag | `False` | è°ƒè¯•å•ä¸ªæ ·æœ¬ |
| `--verbose` | flag | `False` | è¯¦ç»†è¾“å‡º |
| `--save_predictions` | flag | `False` | ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ |

### ä½¿ç”¨ç¤ºä¾‹

#### ç¤ºä¾‹ 1ï¼šå®Œæ•´è¯„ä¼°å¹¶ä¿å­˜é¢„æµ‹

```bash
python run_eval.py --save_predictions
```

#### ç¤ºä¾‹ 2ï¼šè°ƒè¯• TruthfulQA

```bash
python run_eval.py \
    --datasets truthfulqa \
    --max_samples 10 \
    --verbose \
    --debug_sample
```

#### ç¤ºä¾‹ 3ï¼šä½¿ç”¨å®˜æ–¹ LLaMA-3 æ¨¡å‹ï¼ˆéœ€è¦æƒé™ï¼‰

```bash
python run_eval.py \
    --model_name meta-llama/Meta-Llama-3-8B-Instruct
```

#### ç¤ºä¾‹ 4ï¼šCPU æ¨¡å¼

```bash
export CUDA_VISIBLE_DEVICES=""
python run_eval.py --max_samples 100
```

---

## ğŸ“Š è¾“å‡ºè¯´æ˜

### æ§åˆ¶å°è¾“å‡º

è¿è¡Œæ—¶ä¼šå®æ—¶æ˜¾ç¤ºè¯„ä¼°è¿›åº¦å’Œç»“æœï¼š

```
======================================================================
ğŸš€ Priori Judgment Evaluation (CARE Data - Fixed)
======================================================================
Model: NousResearch/Meta-Llama-3-8B-Instruct
Data: data_care/eval
Using: test_question_aware.jsonl (Top-1 only)
Format: answer + topk fields (Fixed)
======================================================================

======================================================================
ğŸ“Š Evaluating NQ
======================================================================
âœ… Loaded 3610 valid samples from nq
ğŸ”„ Evaluating 3610 samples...
Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3610/3610 [12:34<00:00, 4.79it/s]

ğŸ“ˆ Results:
  SPAN_EM: 0.4618 (target: 0.4580) âœ…

ğŸ”€ Mode Distribution:
  rag: 2856/3610 (79.1%)
  closedbook: 754/3610 (20.9%)

...

======================================================================
ğŸ“Š FINAL RESULTS
======================================================================
Dataset         Metric       Result     Target     Status
----------------------------------------------------------------------
nq              SPAN_EM      0.4618     0.4580     âœ…
trivia          SPAN_EM      0.7105     0.7040     âœ…
webqa           SPAN_EM      0.4136     0.4060     âœ…
truthfulqa      F1           0.2589     0.2540     âœ…
truthfulqa      ROUGE_L      0.2345     0.2310     âœ…
factkg          ACCURACY     0.6724     0.6660     âœ…
----------------------------------------------------------------------
AVERAGE                      0.4634     0.4530     âœ…
======================================================================
```

### è¾“å‡ºæ–‡ä»¶

è¿è¡Œå®Œæˆåä¼šåœ¨ `results/` ç›®å½•ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

#### 1. `results.json` - ä¸»ç»“æœæ–‡ä»¶ï¼ˆJSON æ ¼å¼ï¼‰

```json
{
  "nq": {
    "metrics": {
      "span_em": 0.4618
    },
    "mode_distribution": {
      "rag": 2856,
      "closedbook": 754
    }
  },
  "trivia": {
    "metrics": {
      "span_em": 0.7105
    },
    "mode_distribution": {
      "rag": 9245,
      "closedbook": 2068
    }
  },
  ...
}
```

**å­—æ®µè¯´æ˜**ï¼š
- `metrics`: è¯„ä¼°æŒ‡æ ‡ç»“æœ
  - `span_em`: Span Exact Matchï¼ˆå­ä¸²åŒ¹é…ï¼‰
  - `f1`: Token-level F1 score
  - `rouge_l`: ROUGE-L score
  - `accuracy`: äºŒåˆ†ç±»å‡†ç¡®ç‡
- `mode_distribution`: æ¨ç†æ¨¡å¼ç»Ÿè®¡
  - `rag`: ä½¿ç”¨æ£€ç´¢ç»“æœå›ç­”çš„æ ·æœ¬æ•°
  - `closedbook`: å›é€€åˆ°é—­å·æ¨¡å¼çš„æ ·æœ¬æ•°

#### 2. `summary.txt` - æ–‡æœ¬æ±‡æ€»

```
PRIORI JUDGMENT EVALUATION RESULTS
======================================================================

NQ
  SPAN_EM: 0.4618

TRIVIA
  SPAN_EM: 0.7105

WEBQA
  SPAN_EM: 0.4136

TRUTHFULQA
  F1: 0.2589
  ROUGE_L: 0.2345

FACTKG
  ACCURACY: 0.6724

AVERAGE: 0.4634
```

#### 3. `{dataset}_predictions.jsonl` - è¯¦ç»†é¢„æµ‹ï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨ `--save_predictions` å‚æ•°æ—¶ç”Ÿæˆï¼ŒåŒ…å«æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼š

```json
{
  "id": "0",
  "question": "when was the last time anyone was on the moon",
  "prediction": "December 1972",
  "gold_answers": ["14 December 1972 UTC", "December 1972"],
  "mode": "rag",
  "priori_output": "December 1972",
  "correct": true
}
```

**å­—æ®µè¯´æ˜**ï¼š
- `id`: æ ·æœ¬ID
- `question`: é—®é¢˜æ–‡æœ¬
- `prediction`: æ¨¡å‹æœ€ç»ˆé¢„æµ‹
- `gold_answers`: æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨
- `mode`: æ¨ç†æ¨¡å¼ï¼ˆ`rag` æˆ– `closedbook`ï¼‰
- `priori_output`: ç¬¬ä¸€é˜¶æ®µï¼ˆPriori Judgmentï¼‰çš„è¾“å‡º
- `correct`: æ˜¯å¦é¢„æµ‹æ­£ç¡®

---

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### ä¸¤é˜¶æ®µæ¨ç†å®ç°

```python
# Stage 1: Priori Judgment with Top-1 Retrieval
priori_prompt = f"""Given the following information:
{top1_context}

Can you answer the following question based on the given information 
or your internal knowledge? If yes, you should give a short answer with 
one or few words, if no, you should answer "Unknown".

Question: {question}"""

priori_output = model.generate(priori_prompt)

# Stage 2: Fallback to Closed-book if Unknown
if "unknown" in priori_output.lower():
    closedbook_prompt = f"""Answer the questions:
Question: {question}?
The answer is:"""
    
    final_answer = model.generate(closedbook_prompt)
    mode = "closedbook"
else:
    final_answer = priori_output
    mode = "rag"
```

### Prompt æ¨¡æ¿æ¥æº

æ‰€æœ‰ Prompt ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è®ºæ–‡è¡¨æ ¼ï¼š

| Prompt ç±»å‹ | æ¥æº | è¯´æ˜ |
|------------|------|------|
| Priori Judgment (QA) | COLING 2025 Table 6 ç¬¬1è¡Œ | Open-domain & Long-form QA |
| Priori Judgment (Fact) | COLING 2025 Table 6 ç¬¬2è¡Œ | Fact Verification |
| Closed-book (Short QA) | COLING 2025 Table 5 | é—®é¢˜åæœ‰é—®å· |
| Closed-book (Long QA) | COLING 2025 Table 5 | é—®é¢˜åæ— é—®å· |
| Closed-book (Fact) | COLING 2025 Table 5 | è¦æ±‚è¾“å‡º True/False |

### è¯„ä¼°æŒ‡æ ‡è¯¦è§£

#### 1. Span EMï¼ˆSpan Exact Matchï¼‰

**ä¸æ˜¯**ä¸¥æ ¼çš„ token-level exact matchï¼

```python
def compute_span_em(prediction: str, ground_truths: List[str]) -> float:
    pred_normalized = normalize(prediction)
    for gt in ground_truths:
        gt_normalized = normalize(gt)
        # å…³é”®ï¼šå­ä¸²åŒ¹é…
        if gt_normalized in pred_normalized:
            return 1.0
    return 0.0
```

**æ ‡å‡†åŒ–æ­¥éª¤**ï¼š
1. è½¬å°å†™
2. ç§»é™¤å† è¯ï¼ˆa, an, theï¼‰
3. ç§»é™¤æ ‡ç‚¹ç¬¦å·
4. è§„èŒƒåŒ–ç©ºæ ¼

**ç¤ºä¾‹**ï¼š
- Prediction: "The last person on the moon was in December 1972"
- Ground Truth: "December 1972"
- Result: âœ… Matchï¼ˆGT æ˜¯ Pred çš„å­ä¸²ï¼‰

#### 2. Token-level F1

```python
def compute_f1(prediction: str, ground_truths: List[str]) -> float:
    pred_tokens = normalize(prediction).split()
    max_f1 = 0.0
    
    for gt in ground_truths:
        gt_tokens = normalize(gt).split()
        common = Counter(pred_tokens) & Counter(gt_tokens)
        
        precision = common / len(pred_tokens)
        recall = common / len(gt_tokens)
        f1 = 2 * precision * recall / (precision + recall)
        
        max_f1 = max(max_f1, f1)
    
    return max_f1
```

#### 3. ROUGE-L

åŸºäºæœ€é•¿å…¬å…±å­åºåˆ—ï¼ˆLCSï¼‰çš„ F1 åˆ†æ•°ï¼š

```python
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
score = scorer.score(ground_truth, prediction)['rougeL'].fmeasure
```

#### 4. Accuracyï¼ˆFactKGï¼‰

é²æ£’çš„äºŒåˆ†ç±»æ ‡ç­¾æå–ï¼š

```python
def compute_accuracy(prediction: str, ground_truths: List[str]) -> float:
    pred_lower = prediction.lower()
    
    # ä¼˜å…ˆæ£€æŸ¥ falseï¼ˆé¿å… "not true" è¯¯åˆ¤ï¼‰
    if 'false' in pred_lower:
        pred_label = 'false'
    elif 'true' in pred_lower:
        pred_label = 'true'
    else:
        pred_label = 'false'  # é»˜è®¤
    
    gt_label = normalize(ground_truths[0])
    return 1.0 if pred_label == gt_label else 0.0
```

### æ¨¡å‹é…ç½®

```python
model = AutoModelForCausalLM.from_pretrained(
    "NousResearch/Meta-Llama-3-8B-Instruct",
    torch_dtype=torch.float16,  # FP16 ç²¾åº¦
    device_map="auto",          # è‡ªåŠ¨è®¾å¤‡åˆ†é…
    trust_remote_code=True
)

generation_config = {
    "max_new_tokens": 30,       # è®ºæ–‡è®¾ç½®
    "do_sample": False,         # Greedy Decoding
    "pad_token_id": tokenizer.eos_token_id,
    "eos_token_id": tokenizer.eos_token_id
}
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šæ•°æ®æ ¼å¼é”™è¯¯

**ç—‡çŠ¶**ï¼š
```
KeyError: 'answers'
KeyError: 'ctxs'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# 1. éªŒè¯æ•°æ®æ ¼å¼
python test_data_loading.py

# 2. ç¡®è®¤ä½¿ç”¨äº†ä¿®æ­£åçš„ data_loader.py
grep "topk" src/data_loader.py  # åº”è¯¥æ‰¾åˆ°
grep "answer" src/data_loader.py  # åº”è¯¥æ‰¾åˆ°
```

### é—®é¢˜ 2ï¼šCUDA å†…å­˜ä¸è¶³

**ç—‡çŠ¶**ï¼š
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

æ–¹æ³• 1 - ä½¿ç”¨ CPUï¼š
```bash
export CUDA_VISIBLE_DEVICES=""
python run_eval.py
```

æ–¹æ³• 2 - å‡å°‘æ‰¹é‡å¤§å°ï¼ˆä¸é€‚ç”¨äºæœ¬é¡¹ç›®ï¼Œå› ä¸ºæ˜¯å•æ ·æœ¬æ¨ç†ï¼‰

æ–¹æ³• 3 - ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼š
```bash
# éœ€è¦ä¿®æ”¹ evaluator.py æ·»åŠ é‡åŒ–é…ç½®
pip install bitsandbytes
```

### é—®é¢˜ 3ï¼šæ¨¡å‹ä¸‹è½½å¤±è´¥

**ç—‡çŠ¶**ï¼š
```
ConnectionError: Can't connect to huggingface.co
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

æ–¹æ³• 1 - ä½¿ç”¨é•œåƒï¼š
```bash
export HF_ENDPOINT=https://hf-mirror.com
python run_eval.py
```

æ–¹æ³• 2 - æ‰‹åŠ¨ä¸‹è½½ï¼š
```bash
# 1. è®¿é—® https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct
# 2. ä¸‹è½½æ‰€æœ‰æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•
# 3. ä½¿ç”¨æœ¬åœ°è·¯å¾„
python run_eval.py --model_name /path/to/local/model
```

### é—®é¢˜ 4ï¼šåˆ†æ•°æ˜æ˜¾åä½

**ç—‡çŠ¶**ï¼š
- NQ Span EM < 0.40
- TriviaQA Span EM < 0.60

**è¯Šæ–­æ­¥éª¤**ï¼š

æ­¥éª¤ 1 - æ£€æŸ¥å•æ ·æœ¬ï¼š
```bash
python run_eval.py --debug_sample --datasets nq
```

æ­¥éª¤ 2 - æ£€æŸ¥ Prompt æ ¼å¼ï¼š
```bash
grep -A 5 "priori_judgment_qa" src/prompts.py
```

æ­¥éª¤ 3 - æ£€æŸ¥æ•°æ®åŠ è½½ï¼š
```bash
python test_data_loading.py
```

æ­¥éª¤ 4 - æ£€æŸ¥ Unknown æ£€æµ‹ï¼š
```python
# åœ¨ evaluator.py ä¸­æ·»åŠ è°ƒè¯•è¾“å‡º
print(f"Priori output: {priori_output}")
print(f"Is unknown: {self.is_unknown(priori_output)}")
```

### é—®é¢˜ 5ï¼šLLaMA-3 å®˜æ–¹æ¨¡å‹æ— æƒé™

**ç—‡çŠ¶**ï¼š
```
401 Client Error: Unauthorized for url
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

ä½¿ç”¨ç¤¾åŒºç‰ˆæœ¬ï¼ˆé»˜è®¤ï¼‰ï¼š
```bash
# å·²ç»é»˜è®¤ä½¿ç”¨ NousResearch ç‰ˆæœ¬
python run_eval.py
```

æˆ–ç”³è¯·è®¿é—®ï¼š
1. è®¿é—® https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
2. ç‚¹å‡» "Request Access"
3. ç­‰å¾…å®¡æ‰¹ï¼ˆé€šå¸¸å‡ å°æ—¶ï¼‰
4. ä½¿ç”¨ HF token ç™»å½•

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### è®ºæ–‡

1. **CARE: Conflict-Aware Soft Prompting for Retrieval-Augmented Generation**  
   Eunseong Choi, June Park, Hyeri Lee, Jongwuk Lee  
   EMNLP 2025  
   - è®ºæ–‡é“¾æ¥: [arXiv:2508.15253](https://arxiv.org/abs/2508.15253)
   - ä»£ç ä»“åº“: [github.com/eunseongc/CARE](https://github.com/eunseongc/CARE)

2. **Investigating Factual Knowledge Boundary of LLMs with Retrieval Augmentation**  
   Ruiyang Ren et al.  
   COLING 2025
   - Priori Judgment æ–¹æ³•é¦–æ¬¡æå‡º

### æ•°æ®é›†

- **Natural Questions (NQ)**: [ai.google.com/research/NaturalQuestions](https://ai.google.com/research/NaturalQuestions)
- **TriviaQA**: [nlp.cs.washington.edu/triviaqa](http://nlp.cs.washington.edu/triviaqa/)
- **WebQuestions**: [github.com/brmson/dataset-factoid-webquestions](https://github.com/brmson/dataset-factoid-webquestions)
- **TruthfulQA**: [github.com/sylinrl/TruthfulQA](https://github.com/sylinrl/TruthfulQA)
- **FactKG**: From CARE paper

### æ¨¡å‹

- **LLaMA-3**: [huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- **ç¤¾åŒºç‰ˆæœ¬**: [huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct](https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct)

---

## ğŸ¤ è´¡çŒ®ä¸åé¦ˆ

### é¡¹ç›®ç»“æ„

```
priori_judgment_care/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ environment.yml              # Conda ç¯å¢ƒé…ç½®
â”œâ”€â”€ requirements.txt             # Pip ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ verify_data.py               # æ•°æ®éªŒè¯è„šæœ¬
â”œâ”€â”€ test_data_loading.py         # æ•°æ®åŠ è½½æµ‹è¯•
â”œâ”€â”€ run_eval.py                  # ä¸»è¯„ä¼°è„šæœ¬
â”œâ”€â”€ run.sh                       # è¿è¡Œè„šæœ¬ï¼ˆShellï¼‰
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py              # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ data_loader.py           # æ•°æ®åŠ è½½å™¨ï¼ˆå·²ä¿®æ­£ï¼‰
â”‚   â”œâ”€â”€ prompts.py               # Prompt æ¨¡æ¿
â”‚   â”œâ”€â”€ metrics.py               # è¯„ä¼°æŒ‡æ ‡
â”‚   â””â”€â”€ evaluator.py             # ä¸¤é˜¶æ®µæ¨ç†è¯„ä¼°å™¨
â”œâ”€â”€ data_care/eval/              # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ nq/
â”‚   â”œâ”€â”€ triviaqa/
â”‚   â”œâ”€â”€ webqa/
â”‚   â”œâ”€â”€ truthfulqa/
â”‚   â””â”€â”€ factkg/
â””â”€â”€ results/                     # è¾“å‡ºç›®å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
    â”œâ”€â”€ results.json
    â”œâ”€â”€ summary.txt
    â””â”€â”€ {dataset}_predictions.jsonl
```

### å¸¸è§é—®é¢˜

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æŒ‰ä»¥ä¸‹é¡ºåºæ’æŸ¥ï¼š

1. âœ… è¿è¡Œ `python test_data_loading.py` éªŒè¯æ•°æ®
2. âœ… è¿è¡Œ `python verify_data.py` æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
3. âœ… ä½¿ç”¨ `--debug_sample` æŸ¥çœ‹å•æ ·æœ¬æ¨ç†
4. âœ… ä½¿ç”¨ `--max_samples 10` å¿«é€Ÿæµ‹è¯•
5. âœ… æ£€æŸ¥è¾“å‡ºæ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯

### ç‰ˆæœ¬æ›´æ–°

- **v1.1** (2024-01-06): ä¿®æ­£æ•°æ®æ ¼å¼é€‚é…
  - âœ… ä¿®æ­£ `answer` vs `answers` å­—æ®µ
  - âœ… ä¿®æ­£ `topk` vs `ctxs` å­—æ®µ
  - âœ… æ·»åŠ  question_aware æ–‡æœ¬è§£æ
  - âœ… æ·»åŠ è¯¦ç»†è¾“å‡ºå’Œé¢„æµ‹ä¿å­˜

- **v1.0** (2024-01-05): åˆå§‹ç‰ˆæœ¬
  - âœ… åŸºç¡€ä¸¤é˜¶æ®µæ¨ç†å®ç°
  - âœ… 5 ä¸ªæ•°æ®é›†æ”¯æŒ
  - âœ… å®Œæ•´è¯„ä¼°ç®¡é“

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢ CARE è®ºæ–‡ä½œè€…æä¾›æ•°æ®é›†å’ŒåŸºå‡†
- æ„Ÿè°¢ HuggingFace å›¢é˜Ÿæä¾›æ¨¡å‹å’Œå·¥å…·
- æ„Ÿè°¢ NousResearch æä¾›å¼€æº LLaMA-3 æ¨¡å‹

---

## ğŸ“® è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿ï¼š
- æäº¤ GitHub Issue
- å‘é€é‚®ä»¶è‡³é¡¹ç›®ç»´æŠ¤è€…
- å‚è€ƒè®ºæ–‡åŸå§‹ä»“åº“: [github.com/eunseongc/CARE](https://github.com/eunseongc/CARE)

---

**æœ€åæ›´æ–°**: 2024-01-06  
**ç»´æŠ¤è€…**: Priori Judgment Reproduction Team  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª