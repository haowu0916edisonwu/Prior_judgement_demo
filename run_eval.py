#!/usr/bin/env python3
"""
Priori Judgment è¯„ä¼° - ä¿®æ­£ç‰ˆ
é€‚é…å®é™… CARE æ•°æ®æ ¼å¼
"""

import argparse
import json
import os
from pathlib import Path

# éœ€è¦å°†ä¿®æ­£åçš„ data_loader å¤åˆ¶åˆ° src/ ç›®å½•
# è¿™é‡Œå‡è®¾å·²ç»å®Œæˆ
from src.data_loader import CAREDataLoader
from src.evaluator import PrioriJudgmentEvaluator
from src.metrics import Metrics


def debug_single_sample(data_loader, evaluator, dataset):
    """è°ƒè¯•å•ä¸ªæ ·æœ¬"""
    print("\n" + "=" * 70)
    print(f"ğŸ› DEBUG MODE: {dataset.upper()}")
    print("=" * 70)
    
    samples = data_loader.load_dataset(dataset)
    sample = samples[0]
    
    print(f"\nğŸ“ Sample Info:")
    print(f"  ID: {sample.id}")
    print(f"  Question: {sample.question[:200]}...")
    print(f"  Answers: {sample.answers}")
    
    # [ä¿®æ”¹ç‚¹ 1] å°† sample.top1_context æ”¹ä¸º sample.context
    print(f"  Context length: {len(sample.context)} chars")
    # [ä¿®æ”¹ç‚¹ 2] å°† sample.top1_context æ”¹ä¸º sample.context
    print(f"  Context preview: {sample.context[:300]}...")
    
    print(f"\nğŸ”„ Running Two-Stage Inference...")
    result = evaluator.evaluate_sample(sample)
    
    print(f"\nğŸ“Š Results:")
    print(f"  Stage 1 (Priori) Output: {result.priori_output}")
    print(f"  Unknown detected: {evaluator.is_unknown(result.priori_output)}")
    print(f"  Final Answer: {result.prediction}")
    print(f"  Mode: {result.mode}")
    print(f"  Gold Answers: {result.gold_answers}")
    
    # è®¡ç®—æŒ‡æ ‡
    task_type = evaluator.TASK_TYPES[dataset]
    
    if task_type == "fact_checking":
        score = Metrics.compute_accuracy(result.prediction, result.gold_answers)
        metric_name = "Accuracy"
    elif task_type == "long_form":
        score = Metrics.compute_f1(result.prediction, result.gold_answers)
        metric_name = "F1"
    else:
        score = Metrics.compute_span_em(result.prediction, result.gold_answers)
        metric_name = "Span EM"
    
    print(f"\nâœ… {metric_name}: {score:.4f}")
    print("=" * 70)


def save_predictions(results, output_dir, dataset):
    """ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœ (ä¿®å¤ç‰ˆ)"""
    # å…¼å®¹å¤„ç†ï¼šæ— è®º output_dir æ˜¯å­—ç¬¦ä¸²è¿˜æ˜¯ Path å¯¹è±¡éƒ½èƒ½è·‘
    filename = f"{dataset}_predictions.jsonl"
    if hasattr(output_dir, 'joinpath'): # å¦‚æœæ˜¯ Path å¯¹è±¡
        output_file = output_dir / filename
    else: # å¦‚æœæ˜¯å­—ç¬¦ä¸²
        output_file = os.path.join(output_dir, filename)

    print(f"æ­£åœ¨ä¿å­˜è¯¦ç»†é¢„æµ‹åˆ°: {output_file}")

    with open(output_file, 'w', encoding='utf-8') as f:
        for r in results:
            # 1. åŸºç¡€å­—æ®µ
            pred_data = {
                'id': getattr(r, 'id', 'unknown_id'),
                'question': getattr(r, 'question', ''),
                'prediction': getattr(r, 'prediction', ''),
                'gold_answers': getattr(r, 'gold_answers', []),
                'mode': getattr(r, 'mode', 'unknown'),
                'priori_output': getattr(r, 'priori_output', None),
            }

            # 2. æ™ºèƒ½æ¨æ–­ Task Type (Claude çš„é€»è¾‘)
            # å…ˆå°è¯•ç›´æ¥è¯»å– task_typeï¼Œè¯»ä¸åˆ°å†å°è¯•æ¨æ–­
            task_type = getattr(r, 'task_type', None)
            
            # å¦‚æœæ²¡æœ‰ task_typeï¼Œå°è¯•é€šè¿‡ç­”æ¡ˆæ¨æ–­
            if not task_type:
                gold = getattr(r, 'gold_answers', [])
                if len(gold) == 1 and str(gold[0]).lower() in ['true', 'false']:
                    task_type = 'fact_checking'
                else:
                    task_type = 'qa'

            # 3. æ ¹æ®ç±»å‹å®‰å…¨åœ°è¯»å–åˆ†æ•°
            if task_type == "fact_checking":
                # å®‰å…¨è¯»å– accuracyï¼Œé»˜è®¤ False
                pred_data['correct'] = getattr(r, 'accuracy', 0) == 1.0
            else:
                # å®‰å…¨è¯»å– span_emï¼Œé»˜è®¤ False
                pred_data['correct'] = getattr(r, 'span_em', 0) == 1.0

            f.write(json.dumps(pred_data, ensure_ascii=False) + '\n')
    
    print("ä¿å­˜å®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(
        description="Priori Judgment Baseline Evaluation (CARE Data - Fixed)"
    )
    parser.add_argument("--data_root", default="data_care/eval", help="Data directory")
    parser.add_argument("--model_name", default="NousResearch/Meta-Llama-3-8B-Instruct", help="Model name")
    parser.add_argument("--output_dir", default="results", help="Output directory")
    parser.add_argument("--datasets", nargs='+', default=None, help="Datasets to evaluate")
    parser.add_argument("--max_samples", type=int, default=None, help="Limit samples (debug)")
    parser.add_argument("--debug_sample", action="store_true", help="Debug single sample")
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    parser.add_argument("--save_predictions", action="store_true", help="Save detailed predictions")
    args = parser.parse_args()
    
    # æ•°æ®é›†é…ç½®
    all_datasets = ['nq', 'trivia', 'webqa', 'truthfulqa', 'factkg']
    if args.datasets:
        all_datasets = [d for d in all_datasets if d in args.datasets]
    
    # ç›®æ ‡æŒ‡æ ‡
    targets = {
        'nq': {'span_em': 0.458},
        'trivia': {'span_em': 0.704},
        'webqa': {'span_em': 0.406},
        'truthfulqa': {'f1': 0.254, 'rouge_l': 0.231},
        'factkg': {'accuracy': 0.666}
    }
    
    main_metrics = {
        'nq': 'span_em',
        'trivia': 'span_em',
        'webqa': 'span_em',
        'truthfulqa': 'f1',
        'factkg': 'accuracy'
    }
    
    print("=" * 70)
    print("ğŸš€ Priori Judgment Evaluation (CARE Data - Fixed)")
    print("=" * 70)
    print(f"Model: {args.model_name}")
    print(f"Data: {args.data_root}")
    # [ä¿®æ”¹ç‚¹ 3] æ›´æ–°æç¤ºä¿¡æ¯
    print(f"Using: test_question_aware.jsonl (Top-10 Context)") 
    print(f"Format: answer + topk fields (Fixed)")
    print("=" * 70)
    
    # åˆå§‹åŒ–
    data_loader = CAREDataLoader(args.data_root, verbose=args.verbose)
    evaluator = PrioriJudgmentEvaluator(args.model_name)
    
    # Debug mode
    if args.debug_sample:
        for dataset in all_datasets:
            debug_single_sample(data_loader, evaluator, dataset)
        return
    
    # è¯„ä¼°
    all_results = {}
    
    for dataset in all_datasets:
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Evaluating {dataset.upper()}")
        print(f"{'='*70}")
        
        # åŠ è½½æ•°æ®
        samples = data_loader.load_dataset(dataset)
        if args.max_samples:
            samples = samples[:args.max_samples]
            print(f"ğŸ”§ Debug mode: using {len(samples)} samples")
        
        # è¯„ä¼°
        result = evaluator.evaluate_dataset(samples)
        all_results[dataset] = result
        
        # ä¿å­˜è¯¦ç»†é¢„æµ‹
        if args.save_predictions:
            output_dir = Path(args.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            save_predictions(result['results'], output_dir, dataset)
        
        # æ‰“å°ç»“æœ
        metrics = result['metrics']
        print(f"\nğŸ“ˆ Results:")
        for k, v in metrics.items():
            target_v = targets[dataset].get(k, 0)
            status = "âœ…" if v >= target_v - 0.005 else "âŒ"
            print(f"  {k.upper()}: {v:.4f} (target: {target_v:.4f}) {status}")
        
        # æ‰“å°æ¨¡å¼åˆ†å¸ƒ
        print(f"\nğŸ”€ Mode Distribution:")
        for mode, count in result['mode_distribution'].items():
            pct = count / len(samples) * 100
            print(f"  {mode}: {count}/{len(samples)} ({pct:.1f}%)")
    
    # æ±‡æ€»
    print(f"\n{'='*70}")
    print(f"ğŸ“Š FINAL RESULTS")
    print(f"{'='*70}")
    print(f"{'Dataset':<15} {'Metric':<12} {'Result':<10} {'Target':<10} {'Status'}")
    print(f"{'-'*70}")
    
    main_scores = []
    for dataset in all_datasets:
        if dataset not in all_results:
            continue
        
        metrics = all_results[dataset]['metrics']
        for metric_name, metric_value in metrics.items():
            target_value = targets[dataset].get(metric_name, 0)
            status = "âœ…" if metric_value >= target_value - 0.005 else "âŒ"
            
            print(
                f"{dataset:<15} "
                f"{metric_name.upper():<12} "
                f"{metric_value:<10.4f} "
                f"{target_value:<10.4f} "
                f"{status}"
            )
            
            if metric_name == main_metrics[dataset]:
                main_scores.append(metric_value)
    
    # å¹³å‡åˆ†
    if main_scores:
        avg_score = sum(main_scores) / len(main_scores)
        target_avg = 0.453
        status = "âœ…" if avg_score >= target_avg - 0.005 else "âŒ"
        
        print(f"{'-'*70}")
        print(
            f"{'AVERAGE':<15} "
            f"{'':<12} "
            f"{avg_score:<10.4f} "
            f"{target_avg:<10.4f} "
            f"{status}"
        )
    
    print(f"{'='*70}")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    with open(output_dir / "results.json", 'w', encoding='utf-8') as f:
        json.dump({
            dataset: {
                'metrics': result['metrics'],
                'mode_distribution': result['mode_distribution']
            }
            for dataset, result in all_results.items()
        }, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
    with open(output_dir / "summary.txt", 'w', encoding='utf-8') as f:
        f.write("PRIORI JUDGMENT EVALUATION RESULTS\n")
        f.write("=" * 70 + "\n\n")
        
        for dataset in all_datasets:
            if dataset not in all_results:
                continue
            f.write(f"{dataset.upper()}\n")
            for k, v in all_results[dataset]['metrics'].items():
                f.write(f"  {k.upper()}: {v:.4f}\n")
            f.write("\n")
        
        if main_scores:
            f.write(f"AVERAGE: {avg_score:.4f}\n")
    
    print(f"\nğŸ’¾ Results saved:")
    print(f"  - {output_dir}/results.json (ä¸»ç»“æœ)")
    print(f"  - {output_dir}/summary.txt (æ–‡æœ¬æ±‡æ€»)")
    if args.save_predictions:
        print(f"  - {output_dir}/{{dataset}}_predictions.jsonl (è¯¦ç»†é¢„æµ‹)")


if __name__ == "__main__":
    main()
