#!/usr/bin/env python3
"""
Priori Judgment è¯„ä¼° - ä¿®æ­£ç‰ˆ (v25.0 Final)
é€‚é…å®é™… CARE æ•°æ®æ ¼å¼
ä¿®æ­£ç®—åˆ†é€»è¾‘ï¼šTruthfulQA è®¡å…¥ F1 å’Œ ROUGE ä¸¤é¡¹ï¼Œæ€»åˆ†æ¯ä¸º 6ã€‚
"""

import argparse
import json
import os
from pathlib import Path

# éœ€è¦å°†ä¿®æ­£åçš„ data_loader å¤åˆ¶åˆ° src/ ç›®å½•
# è¿™é‡Œå‡è®¾å·²ç»å®Œæˆ
from src.data_loader import CAREDataLoader
from src.evaluator import PrioriJudgmentEvaluator
from src.metrics import Metrics


def debug_single_sample(data_loader, evaluator, dataset):
    """è°ƒè¯•å•ä¸ªæ ·æœ¬"""
    print("\n" + "=" * 70)
    print(f"ğŸ› DEBUG MODE: {dataset.upper()}")
    print("=" * 70)
    
    samples = data_loader.load_dataset(dataset)
    sample = samples[0]
    
    print(f"\nğŸ“ Sample Info:")
    print(f"  ID: {sample.id}")
    print(f"  Question: {sample.question[:200]}...")
    print(f"  Answers: {sample.answers}")
    
    # [ä¿®æ”¹ç‚¹ 1] å°† sample.top1_context æ”¹ä¸º sample.context
    print(f"  Context length: {len(sample.context)} chars")
    # [ä¿®æ”¹ç‚¹ 2] å°† sample.top1_context æ”¹ä¸º sample.context
    print(f"  Context preview: {sample.context[:300]}...")
    
    print(f"\nğŸ”„ Running Two-Stage Inference...")
    result = evaluator.evaluate_sample(sample)
    
    print(f"\nğŸ“Š Results:")
    print(f"  Stage 1 (Priori) Output: {result.priori_output}")
    # print(f"  Unknown detected: {evaluator.is_unknown(result.priori_output)}") # evaluator å¯èƒ½æ²¡æœ‰å…¬å¼€è¿™ä¸ªæ–¹æ³•ï¼Œæ³¨é‡Šæ‰ä»¥é˜²ä¸‡ä¸€
    print(f"  Final Answer: {result.prediction}")
    print(f"  Mode: {result.mode}")
    print(f"  Gold Answers: {result.gold_answers}")
    
    # è®¡ç®—æŒ‡æ ‡
    task_type = evaluator.TASK_TYPES.get(dataset, 'open_qa')
    
    if task_type == "fact_checking":
        score = Metrics.compute_accuracy(result.prediction, result.gold_answers)
        metric_name = "Accuracy"
    elif task_type == "long_form":
        score = Metrics.compute_f1(result.prediction, result.gold_answers)
        metric_name = "F1"
    else:
        score = Metrics.compute_span_em(result.prediction, result.gold_answers)
        metric_name = "Span EM"
    
    print(f"\nâœ… {metric_name}: {score:.4f}")
    print("=" * 70)


def save_predictions(results, output_dir, dataset):
    """ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœ (ä¿®å¤ç‰ˆ)"""
    # å…¼å®¹å¤„ç†ï¼šæ— è®º output_dir æ˜¯å­—ç¬¦ä¸²è¿˜æ˜¯ Path å¯¹è±¡éƒ½èƒ½è·‘
    filename = f"{dataset}_predictions.jsonl"
    if hasattr(output_dir, 'joinpath'): # å¦‚æœæ˜¯ Path å¯¹è±¡
        output_file = output_dir / filename
    else: # å¦‚æœæ˜¯å­—ç¬¦ä¸²
        output_file = os.path.join(output_dir, filename)

    print(f"æ­£åœ¨ä¿å­˜è¯¦ç»†é¢„æµ‹åˆ°: {output_file}")

    with open(output_file, 'w', encoding='utf-8') as f:
        for r in results:
            # 1. åŸºç¡€å­—æ®µ
            pred_data = {
                'id': getattr(r, 'id', 'unknown_id'),
                'question': getattr(r, 'question', ''),
                'prediction': getattr(r, 'prediction', ''),
                'gold_answers': getattr(r, 'gold_answers', []),
                'mode': getattr(r, 'mode', 'unknown'),
                'priori_output': getattr(r, 'priori_output', None),
            }

            # 2. æ™ºèƒ½æ¨æ–­ Task Type
            task_type = getattr(r, 'task_type', None)
            
            # å¦‚æœæ²¡æœ‰ task_typeï¼Œå°è¯•é€šè¿‡ç­”æ¡ˆæ¨æ–­
            if not task_type:
                gold = getattr(r, 'gold_answers', [])
                if len(gold) == 1 and str(gold[0]).lower() in ['true', 'false']:
                    task_type = 'fact_checking'
                else:
                    task_type = 'qa'

            # 3. æ ¹æ®ç±»å‹å®‰å…¨åœ°è¯»å–åˆ†æ•°
            if task_type == "fact_checking":
                pred_data['correct'] = getattr(r, 'accuracy', 0) == 1.0
            else:
                pred_data['correct'] = getattr(r, 'span_em', 0) == 1.0

            f.write(json.dumps(pred_data, ensure_ascii=False) + '\n')
    
    print("ä¿å­˜å®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(
        description="Priori Judgment Baseline Evaluation (CARE Data - Fixed)"
    )
    parser.add_argument("--data_root", default="data_care/eval", help="Data directory")
    parser.add_argument("--model_name", default="NousResearch/Meta-Llama-3-8B-Instruct", help="Model name")
    parser.add_argument("--output_dir", default="results", help="Output directory")
    parser.add_argument("--datasets", nargs='+', default=None, help="Datasets to evaluate")
    parser.add_argument("--max_samples", type=int, default=None, help="Limit samples (debug)")
    parser.add_argument("--debug_sample", action="store_true", help="Debug single sample")
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    parser.add_argument("--save_predictions", action="store_true", help="Save detailed predictions")
    args = parser.parse_args()
    
    # æ•°æ®é›†é…ç½®
    all_datasets = ['nq', 'trivia', 'webqa', 'truthfulqa', 'factkg']
    if args.datasets:
        all_datasets = [d for d in all_datasets if d in args.datasets]
    
    # [å…³é”®ä¿®æ”¹] è®ºæ–‡æ ‡å‡†æŒ‡æ ‡åˆ—è¡¨ (6é¡¹)
    # æ ¼å¼: (Dataset, Metric Key, Target Value)
    paper_metrics_def = [
        ('nq', 'span_em', 0.458),
        ('trivia', 'span_em', 0.704),
        ('webqa', 'span_em', 0.406),
        ('truthfulqa', 'f1', 0.254),       # TQA Item 1
        ('truthfulqa', 'rouge_l', 0.231),  # TQA Item 2
        ('factkg', 'accuracy', 0.666)
    ]
    
    # ç”¨äºå•é¡¹å±•ç¤ºçš„ç›®æ ‡å­—å…¸
    targets_map = {
        'nq': {'span_em': 0.458},
        'trivia': {'span_em': 0.704},
        'webqa': {'span_em': 0.406},
        'truthfulqa': {'f1': 0.254, 'rouge_l': 0.231},
        'factkg': {'accuracy': 0.666}
    }
    
    print("=" * 70)
    print("ğŸš€ Priori Judgment Evaluation (CARE Data - Fixed)")
    print("=" * 70)
    print(f"Model: {args.model_name}")
    print(f"Data: {args.data_root}")
    print(f"Using: test_question_aware.jsonl (Top-10 Context)") 
    print(f"Format: Answer + Topk Fields")
    print("=" * 70)
    
    # åˆå§‹åŒ–
    data_loader = CAREDataLoader(args.data_root, verbose=args.verbose)
    evaluator = PrioriJudgmentEvaluator(args.model_name)
    
    # Debug mode
    if args.debug_sample:
        for dataset in all_datasets:
            debug_single_sample(data_loader, evaluator, dataset)
        return
    
    # è¯„ä¼°
    all_results = {}
    
    for dataset in all_datasets:
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Evaluating {dataset.upper()}")
        print(f"{'='*70}")
        
        # åŠ è½½æ•°æ®
        samples = data_loader.load_dataset(dataset)
        if args.max_samples:
            samples = samples[:args.max_samples]
            print(f"ğŸ”§ Debug mode: using {len(samples)} samples")
        
        # è¯„ä¼°
        result = evaluator.evaluate_dataset(samples)
        all_results[dataset] = result
        
        # ä¿å­˜è¯¦ç»†é¢„æµ‹
        if args.save_predictions:
            output_dir = Path(args.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            save_predictions(result['results'], output_dir, dataset)
        
        # æ‰“å°ç»“æœ
        metrics = result['metrics']
        print(f"\nğŸ“ˆ Results:")
        for k, v in metrics.items():
            target_v = targets_map[dataset].get(k, 0)
            status = "âœ…" if v >= target_v - 0.005 else "âŒ"
            print(f"  {k.upper()}: {v:.4f} (target: {target_v:.4f}) {status}")
        
        # æ‰“å°æ¨¡å¼åˆ†å¸ƒ
        print(f"\nğŸ”€ Mode Distribution:")
        for mode, count in result['mode_distribution'].items():
            pct = count / len(samples) * 100
            print(f"  {mode}: {count}/{len(samples)} ({pct:.1f}%)")
    
    # ======================================================================
    # ğŸ“Š FINAL RESULTS (Strict Format & Logic)
    # ======================================================================
    print(f"\n{'='*70}")
    print(f"ğŸ“Š FINAL RESULTS")
    print(f"{'='*70}")
    print(f"{'Dataset':<15} {'Metric':<12} {'Result':<10} {'Target':<10} {'Status'}")
    print(f"{'-'*70}")
    
    collected_scores = []
    
    # æŒ‰ç…§å®šä¹‰çš„é¡ºåºéå† 6 é¡¹æŒ‡æ ‡
    for ds_name, metric_key, target_val in paper_metrics_def:
        # æ£€æŸ¥æ˜¯å¦è·‘äº†è¿™ä¸ªæ•°æ®é›†
        if ds_name in all_results and metric_key in all_results[ds_name]['metrics']:
            score = all_results[ds_name]['metrics'][metric_key]
            collected_scores.append(score)
            
            status = "âœ…" if score >= target_val - 0.005 else "âŒ"
            
            print(
                f"{ds_name:<15} "
                f"{metric_key.upper():<12} "
                f"{score:<10.4f} "
                f"{target_val:<10.4f} "
                f"{status}"
            )
        else:
            # å¦‚æœæ²¡è·‘ï¼Œæ˜¾ç¤º N/A
            print(
                f"{ds_name:<15} "
                f"{metric_key.upper():<12} "
                f"{'N/A':<10} "
                f"{target_val:<10.4f} "
                f"âšª"
            )

    print(f"{'-'*70}")
    
    # è®¡ç®—å¹³å‡åˆ† (Standard: Div by 6)
    final_avg = 0.0
    avg_status = ""
    
    if len(collected_scores) == 6:
        final_avg = sum(collected_scores) / 6
        target_avg = 0.453
        status_icon = "âœ…" if final_avg >= target_avg - 0.005 else "âŒ"
        
        print(
            f"{'AVERAGE':<15} "
            f"{'':<12} "
            f"{final_avg:<10.4f} "
            f"{target_avg:<10.4f} "
            f"{status_icon}"
        )
    elif collected_scores:
        # éƒ¨åˆ†è¿è¡Œçš„æƒ…å†µ
        final_avg = sum(collected_scores) / len(collected_scores)
        print(
            f"{'PARTIAL AVG':<15} "
            f"{'(Div/'+str(len(collected_scores))+')':<12} "
            f"{final_avg:<10.4f} "
            f"{'0.4530':<10} "
            f"âš ï¸"
        )
    else:
        print("No scores collected.")

    print(f"{'='*70}")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ (JSON)
    with open(output_dir / "results.json", 'w', encoding='utf-8') as f:
        json.dump({
            dataset: {
                'metrics': result['metrics'],
                'mode_distribution': result.get('mode_distribution', {})
            }
            for dataset, result in all_results.items()
        }, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æ±‡æ€»ç»Ÿè®¡ (TXT)
    with open(output_dir / "summary.txt", 'w', encoding='utf-8') as f:
        f.write("PRIORI JUDGMENT EVALUATION RESULTS\n")
        f.write("=" * 70 + "\n\n")
        
        # å†™å…¥æ¯ä¸ªæ•°æ®é›†çš„è¯¦ç»†åˆ†
        for dataset in all_datasets:
            if dataset not in all_results:
                continue
            f.write(f"{dataset.upper()}\n")
            for k, v in all_results[dataset]['metrics'].items():
                f.write(f"  {k.upper()}: {v:.4f}\n")
            f.write("\n")
        
        # å†™å…¥æœ€ç»ˆå¹³å‡åˆ†
        if collected_scores:
            label = "AVERAGE" if len(collected_scores) == 6 else "PARTIAL AVG"
            f.write(f"{label}: {final_avg:.4f}\n")
    
    print(f"\nğŸ’¾ Results saved:")
    print(f"  - {output_dir}/results.json (ä¸»ç»“æœ)")
    print(f"  - {output_dir}/summary.txt (æ–‡æœ¬æ±‡æ€»)")
    if args.save_predictions:
        print(f"  - {output_dir}/{{dataset}}_predictions.jsonl (è¯¦ç»†é¢„æµ‹)")


if __name__ == "__main__":
    main()